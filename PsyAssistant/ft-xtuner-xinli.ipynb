{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/EmoLLM\n!git clone https://github.com/SmartFlowAI/EmoLLM.git        ","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:56:11.975831Z","iopub.execute_input":"2024-04-13T11:56:11.976550Z","iopub.status.idle":"2024-04-13T11:56:20.115373Z","shell.execute_reply.started":"2024-04-13T11:56:11.976518Z","shell.execute_reply":"2024-04-13T11:56:20.114133Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Cloning into 'EmoLLM'...\nremote: Enumerating objects: 2016, done.\u001b[K\nremote: Counting objects: 100% (532/532), done.\u001b[K\nremote: Compressing objects: 100% (258/258), done.\u001b[K\nremote: Total 2016 (delta 373), reused 340 (delta 274), pack-reused 1484\u001b[K\nReceiving objects: 100% (2016/2016), 86.82 MiB | 27.39 MiB/s, done.\nResolving deltas: 100% (1087/1087), done.\nUpdating files: 100% (215/215), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U 'xtuner[deepspeed]'","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:50:12.243452Z","iopub.execute_input":"2024-04-13T11:50:12.244171Z","iopub.status.idle":"2024-04-13T11:51:10.656705Z","shell.execute_reply.started":"2024-04-13T11:50:12.244140Z","shell.execute_reply":"2024-04-13T11:51:10.655578Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting xtuner[deepspeed]\n  Downloading xtuner-0.1.17-py3-none-any.whl.metadata (18 kB)\nCollecting bitsandbytes>=0.40.0.post4 (from xtuner[deepspeed])\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from xtuner[deepspeed]) (2.18.0)\nCollecting einops (from xtuner[deepspeed])\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nCollecting lagent>=0.1.2 (from xtuner[deepspeed])\n  Downloading lagent-0.2.2-py3-none-any.whl.metadata (5.9 kB)\nCollecting mmengine>=0.10.3 (from xtuner[deepspeed])\n  Downloading mmengine-0.10.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (from xtuner[deepspeed]) (3.1.2)\nRequirement already satisfied: peft>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from xtuner[deepspeed]) (0.9.0)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from xtuner[deepspeed]) (0.22.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xtuner[deepspeed]) (1.11.4)\nRequirement already satisfied: SentencePiece in /opt/conda/lib/python3.10/site-packages (from xtuner[deepspeed]) (0.2.0)\nRequirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from xtuner[deepspeed]) (0.6.0)\nRequirement already satisfied: torch<=2.1.2 in /opt/conda/lib/python3.10/site-packages (from xtuner[deepspeed]) (2.1.2)\nRequirement already satisfied: torchvision<=0.16.2 in /opt/conda/lib/python3.10/site-packages (from xtuner[deepspeed]) (0.16.2)\nCollecting transformers!=4.38.0,!=4.38.1,!=4.38.2,>=4.36.0 (from xtuner[deepspeed])\n  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting transformers-stream-generator (from xtuner[deepspeed])\n  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting deepspeed>=0.12.3 (from xtuner[deepspeed])\n  Downloading deepspeed-0.14.0.tar.gz (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting mpi4py-mpich (from xtuner[deepspeed])\n  Downloading mpi4py_mpich-3.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes>=0.40.0.post4->xtuner[deepspeed]) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets>=2.16.0->xtuner[deepspeed]) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->xtuner[deepspeed]) (6.0.1)\nCollecting hjson (from deepspeed>=0.12.3->xtuner[deepspeed])\n  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed>=0.12.3->xtuner[deepspeed]) (1.11.1.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed>=0.12.3->xtuner[deepspeed]) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed>=0.12.3->xtuner[deepspeed]) (9.0.0)\nRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed>=0.12.3->xtuner[deepspeed]) (2.5.3)\nRequirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from deepspeed>=0.12.3->xtuner[deepspeed]) (11.4.1)\nCollecting arxiv (from lagent>=0.1.2->xtuner[deepspeed])\n  Downloading arxiv-2.1.0-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: distro in /opt/conda/lib/python3.10/site-packages (from lagent>=0.1.2->xtuner[deepspeed]) (1.9.0)\nCollecting func-timeout (from lagent>=0.1.2->xtuner[deepspeed])\n  Downloading func_timeout-4.3.5.tar.gz (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting griffe (from lagent>=0.1.2->xtuner[deepspeed])\n  Downloading griffe-0.42.1-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: json5 in /opt/conda/lib/python3.10/site-packages (from lagent>=0.1.2->xtuner[deepspeed]) (0.9.14)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from lagent>=0.1.2->xtuner[deepspeed]) (4.20.0)\nCollecting jupyter (from lagent>=0.1.2->xtuner[deepspeed])\n  Downloading jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\nRequirement already satisfied: jupyter-client in /opt/conda/lib/python3.10/site-packages (from lagent>=0.1.2->xtuner[deepspeed]) (7.4.9)\nCollecting phx-class-registry (from lagent>=0.1.2->xtuner[deepspeed])\n  Downloading phx_class_registry-4.1.0-py3-none-any.whl.metadata (8.2 kB)\nCollecting streamlit (from lagent>=0.1.2->xtuner[deepspeed])\n  Downloading streamlit-1.33.0-py2.py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lagent>=0.1.2->xtuner[deepspeed]) (4.9.0)\nRequirement already satisfied: addict in /opt/conda/lib/python3.10/site-packages (from mmengine>=0.10.3->xtuner[deepspeed]) (2.4.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from mmengine>=0.10.3->xtuner[deepspeed]) (3.7.5)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from mmengine>=0.10.3->xtuner[deepspeed]) (13.7.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from mmengine>=0.10.3->xtuner[deepspeed]) (2.4.0)\nRequirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from mmengine>=0.10.3->xtuner[deepspeed]) (0.40.2)\nRequirement already satisfied: opencv-python>=3 in /opt/conda/lib/python3.10/site-packages (from mmengine>=0.10.3->xtuner[deepspeed]) (4.9.0.80)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft>=0.4.0->xtuner[deepspeed]) (0.28.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft>=0.4.0->xtuner[deepspeed]) (0.4.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<=2.1.2->xtuner[deepspeed]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<=2.1.2->xtuner[deepspeed]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<=2.1.2->xtuner[deepspeed]) (3.1.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision<=0.16.2->xtuner[deepspeed]) (9.5.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers!=4.38.0,!=4.38.1,!=4.38.2,>=4.36.0->xtuner[deepspeed]) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers!=4.38.0,!=4.38.1,!=4.38.2,>=4.36.0->xtuner[deepspeed]) (0.15.2)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl->xtuner[deepspeed]) (1.1.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->xtuner[deepspeed]) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->xtuner[deepspeed]) (2023.12.9)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image->xtuner[deepspeed]) (0.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->xtuner[deepspeed]) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->xtuner[deepspeed]) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->xtuner[deepspeed]) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->xtuner[deepspeed]) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->xtuner[deepspeed]) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->xtuner[deepspeed]) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets>=2.16.0->xtuner[deepspeed]) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->xtuner[deepspeed]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->xtuner[deepspeed]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->xtuner[deepspeed]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.16.0->xtuner[deepspeed]) (2024.2.2)\nCollecting feedparser==6.0.10 (from arxiv->lagent>=0.1.2->xtuner[deepspeed])\n  Downloading feedparser-6.0.10-py3-none-any.whl.metadata (2.3 kB)\nCollecting sgmllib3k (from feedparser==6.0.10->arxiv->lagent>=0.1.2->xtuner[deepspeed])\n  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: colorama>=0.4 in /opt/conda/lib/python3.10/site-packages (from griffe->lagent>=0.1.2->xtuner[deepspeed]) (0.4.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<=2.1.2->xtuner[deepspeed]) (2.1.3)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->lagent>=0.1.2->xtuner[deepspeed]) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->lagent>=0.1.2->xtuner[deepspeed]) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->lagent>=0.1.2->xtuner[deepspeed]) (0.16.2)\nRequirement already satisfied: notebook in /opt/conda/lib/python3.10/site-packages (from jupyter->lagent>=0.1.2->xtuner[deepspeed]) (6.5.4)\nRequirement already satisfied: qtconsole in /opt/conda/lib/python3.10/site-packages (from jupyter->lagent>=0.1.2->xtuner[deepspeed]) (5.5.1)\nRequirement already satisfied: jupyter-console in /opt/conda/lib/python3.10/site-packages (from jupyter->lagent>=0.1.2->xtuner[deepspeed]) (6.6.3)\nRequirement already satisfied: nbconvert in /opt/conda/lib/python3.10/site-packages (from jupyter->lagent>=0.1.2->xtuner[deepspeed]) (6.4.5)\nRequirement already satisfied: ipykernel in /opt/conda/lib/python3.10/site-packages (from jupyter->lagent>=0.1.2->xtuner[deepspeed]) (6.28.0)\nRequirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (from jupyter->lagent>=0.1.2->xtuner[deepspeed]) (7.7.1)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client->lagent>=0.1.2->xtuner[deepspeed]) (0.4)\nRequirement already satisfied: jupyter-core>=4.9.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-client->lagent>=0.1.2->xtuner[deepspeed]) (5.7.1)\nRequirement already satisfied: nest-asyncio>=1.5.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-client->lagent>=0.1.2->xtuner[deepspeed]) (1.5.8)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-client->lagent>=0.1.2->xtuner[deepspeed]) (2.9.0.post0)\nRequirement already satisfied: pyzmq>=23.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-client->lagent>=0.1.2->xtuner[deepspeed]) (24.0.1)\nRequirement already satisfied: tornado>=6.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-client->lagent>=0.1.2->xtuner[deepspeed]) (6.3.3)\nRequirement already satisfied: traitlets in /opt/conda/lib/python3.10/site-packages (from jupyter-client->lagent>=0.1.2->xtuner[deepspeed]) (5.9.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine>=0.10.3->xtuner[deepspeed]) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine>=0.10.3->xtuner[deepspeed]) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine>=0.10.3->xtuner[deepspeed]) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->mmengine>=0.10.3->xtuner[deepspeed]) (1.4.5)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->xtuner[deepspeed]) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->xtuner[deepspeed]) (2023.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed>=0.12.3->xtuner[deepspeed]) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic->deepspeed>=0.12.3->xtuner[deepspeed]) (2.14.6)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->mmengine>=0.10.3->xtuner[deepspeed]) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->mmengine>=0.10.3->xtuner[deepspeed]) (2.17.2)\nRequirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->lagent>=0.1.2->xtuner[deepspeed]) (5.3.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->lagent>=0.1.2->xtuner[deepspeed]) (1.7.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->lagent>=0.1.2->xtuner[deepspeed]) (4.2.4)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->lagent>=0.1.2->xtuner[deepspeed]) (8.1.7)\nRequirement already satisfied: protobuf<5,>=3.20 in /opt/conda/lib/python3.10/site-packages (from streamlit->lagent>=0.1.2->xtuner[deepspeed]) (3.20.3)\nRequirement already satisfied: tenacity<9,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->lagent>=0.1.2->xtuner[deepspeed]) (8.2.3)\nRequirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit->lagent>=0.1.2->xtuner[deepspeed]) (0.10.2)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit->lagent>=0.1.2->xtuner[deepspeed]) (3.1.41)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit->lagent>=0.1.2->xtuner[deepspeed])\n  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl.metadata (3.9 kB)\nCollecting watchdog>=2.1.5 (from streamlit->lagent>=0.1.2->xtuner[deepspeed])\n  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<=2.1.2->xtuner[deepspeed]) (1.3.0)\nRequirement already satisfied: importlib-metadata>=6.6.0 in /opt/conda/lib/python3.10/site-packages (from yapf->mmengine>=0.10.3->xtuner[deepspeed]) (6.11.0)\nRequirement already satisfied: platformdirs>=3.5.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmengine>=0.10.3->xtuner[deepspeed]) (4.2.0)\nRequirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmengine>=0.10.3->xtuner[deepspeed]) (2.0.1)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->lagent>=0.1.2->xtuner[deepspeed]) (0.12.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->lagent>=0.1.2->xtuner[deepspeed]) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=6.6.0->yapf->mmengine>=0.10.3->xtuner[deepspeed]) (3.17.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->mmengine>=0.10.3->xtuner[deepspeed]) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client->lagent>=0.1.2->xtuner[deepspeed]) (1.16.0)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.2.1)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (1.8.0)\nRequirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (8.20.0)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.1.6)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.2.0)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (3.6.6)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (3.0.9)\nRequirement already satisfied: prompt-toolkit>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from jupyter-console->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (3.0.42)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.3.0)\nRequirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (5.9.2)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (6.1.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (1.5.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.6.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (4.12.2)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.5.13)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (23.1.0)\nRequirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (1.8.2)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.18.0)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.19.0)\nRequirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (1.0.0)\nRequirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from qtconsole->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (2.4.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->lagent>=0.1.2->xtuner[deepspeed]) (5.0.1)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.19.1)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (4.8.0)\nRequirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (2.13.0)\nRequirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.2.3)\nRequirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.4->nbconvert->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (2.19.1)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.2.13)\nRequirement already satisfied: ptyprocess in /opt/conda/lib/python3.10/site-packages (from terminado>=0.8.3->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.7.0)\nRequirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (21.2.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (2.5)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.5.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.8.3)\nRequirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (4.2.0)\nRequirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.9.0)\nRequirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.5.1)\nRequirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (7.4.0)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (1.7.0)\nRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (1.16.0)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.2.2)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (1.3.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (2.21)\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (2.0.7)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (0.1.1)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (2.4)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (1.3.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (1.13)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (1.3.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->lagent>=0.1.2->xtuner[deepspeed]) (2.8.19.20240106)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lagent-0.2.2-py3-none-any.whl (69 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmengine-0.10.3-py3-none-any.whl (451 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.7/451.7 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mpi4py_mpich-3.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading xtuner-0.1.17-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading arxiv-2.1.0-py3-none-any.whl (11 kB)\nDownloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading griffe-0.42.1-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\nDownloading phx_class_registry-4.1.0-py3-none-any.whl (13 kB)\nDownloading streamlit-1.33.0-py2.py3-none-any.whl (8.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: deepspeed, transformers-stream-generator, func-timeout, sgmllib3k\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400391 sha256=90475009a04fb283f69ab2ce83e36235febab9e3805c195b6fc5b5c9480f80c4\n  Stored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\n  Building wheel for transformers-stream-generator (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12424 sha256=0cc0e1dc233c1886a63a65cab9725f3548f1cf725c2235d768751cff1e486b60\n  Stored in directory: /root/.cache/pip/wheels/95/4a/90/140f7b67d125906f6a165f38aad212ecb4a695ad0d87582437\n  Building wheel for func-timeout (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for func-timeout: filename=func_timeout-4.3.5-py3-none-any.whl size=15076 sha256=da64639d26eaeb2d3e7db0cacb2b898c76912537fdbfb3e495d88d3d90c75577\n  Stored in directory: /root/.cache/pip/wheels/3f/83/19/b5552bb9630e353f7c5b15be44bf10900afe1abbbfcf536afd\n  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=2b083a6ad429f56c9e4e5237867fe1abf23a1448a9415e51836f0a542116d5d9\n  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\nSuccessfully built deepspeed transformers-stream-generator func-timeout sgmllib3k\nInstalling collected packages: sgmllib3k, hjson, func-timeout, watchdog, phx-class-registry, mpi4py-mpich, griffe, feedparser, einops, pydeck, arxiv, mmengine, deepspeed, bitsandbytes, transformers, transformers-stream-generator, streamlit, jupyter, lagent, xtuner\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.2\n    Uninstalling transformers-4.38.2:\n      Successfully uninstalled transformers-4.38.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlmdeploy 0.3.0 requires transformers<=4.38.2,>=4.33.0, but you have transformers 4.39.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed arxiv-2.1.0 bitsandbytes-0.43.1 deepspeed-0.14.0 einops-0.7.0 feedparser-6.0.10 func-timeout-4.3.5 griffe-0.42.1 hjson-3.1.0 jupyter-1.0.0 lagent-0.2.2 mmengine-0.10.3 mpi4py-mpich-3.1.5 phx-class-registry-4.1.0 pydeck-0.8.1b0 sgmllib3k-1.0.0 streamlit-1.33.0 transformers-4.39.3 transformers-stream-generator-0.0.5 watchdog-4.0.0 xtuner-0.1.17\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip show xtuner","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:52:57.724408Z","iopub.execute_input":"2024-04-13T11:52:57.725110Z","iopub.status.idle":"2024-04-13T11:53:09.711817Z","shell.execute_reply.started":"2024-04-13T11:52:57.725074Z","shell.execute_reply":"2024-04-13T11:53:09.710881Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Name: xtuner\nVersion: 0.1.17\nSummary: An efficient, flexible and full-featured toolkit for fine-tuning large models\nHome-page: https://github.com/InternLM/xtuner\nAuthor: XTuner Contributors\nAuthor-email: openmmlab@gmail.com\nLicense: Apache License 2.0\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: bitsandbytes, datasets, einops, lagent, mmengine, openpyxl, peft, scikit-image, scipy, SentencePiece, tiktoken, torch, torchvision, transformers, transformers-stream-generator\nRequired-by: \n","output_type":"stream"}]},{"cell_type":"code","source":"#!huggingface-cli download --resume-download internlm/internlm2-chat-7b --local-dir /root/share/model_repos/internlm2-chat-7b","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Qwen 0.5 full \n-**batch=4,max_length=512 , GPU=14.5G**  \n最大内存 = batch * max_length \n例如： 4 * 512 = 1 * 2048\n","metadata":{}},{"cell_type":"code","source":"!huggingface-cli download --resume-download Qwen/Qwen1.5-0.5B-Chat --local-dir /kaggle/working/Qwen\nconfig_file = \"/kaggle/working/EmoLLM/xtuner_config/qwen1_5_0_5_B_full.py\"\n\n# # 首先读取文件内容\n# with open(config_file, 'r') as file:\n#     content = file.read()\n\n# # 修改config.ini的内容 ，转换bfloat16为float16   \n# content = content.replace('./data_pro.json', '/kaggle/working/EmoLLM/datasets/data_pro.json')\n# content = content.replace('/root/model/qwen/Qwen1___5-0___5B-Chat','/kaggle/working/Qwen')\n# content = content.replace('load_from = \\'/root/Emollm/work_dirs/qwen_0_5_B/iter_255.pth\\'','load_from = None')\n\n# content = content.replace('batch_size = 4  # per_device','batch_size = 1  # per_device')\n# content = content.replace('max_length = 1024','max_length = 512')\n\n# print(content)\n# # # 写入新内容\n# with open(config_file, 'w') as file:\n#     file.write(content)\n\n# 打开并读取config.txt文件\nwith open(config_file, 'r') as file:\n    lines = file.readlines()\n\nlines[22] = 'pretrained_model_name_or_path = \"/kaggle/working/Qwen\"\\n'\nlines[26] = 'data_path = \"/kaggle/working/EmoLLM/datasets/data_pro.json\"\\n'  # 注意索引是从0开始\nlines[28] = 'max_length = 2048\\n'  # 注意索引是从0开始\nlines[32] = 'batch_size = 1  # per_device\\n'  # 注意索引是从0开始\nlines[33] = 'accumulative_counts = 64\\n'\nlines[182] = 'load_from = None\\n'\n\n# 回写到config.txt文件\nwith open(config_file, 'w') as file:\n    file.writelines(lines)\n    \nwith open(config_file, 'r') as file:\n     content = file.read()    \nprint(content)        ","metadata":{"execution":{"iopub.status.busy":"2024-04-13T10:51:28.254817Z","iopub.execute_input":"2024-04-13T10:51:28.255666Z","iopub.status.idle":"2024-04-13T10:51:36.272609Z","shell.execute_reply.started":"2024-04-13T10:51:28.255626Z","shell.execute_reply":"2024-04-13T10:51:36.271530Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\nFetching 10 files:   0%|                                 | 0/10 [00:00<?, ?it/s]downloading https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/resolve/f82bd3692de0283f4a4b31e06d164dd8467fb52e/config.json to /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B-Chat/blobs/f0041555808f66af0491fa912882f59ed29ed414.incomplete\ndownloading https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/resolve/f82bd3692de0283f4a4b31e06d164dd8467fb52e/tokenizer.json to /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B-Chat/blobs/33ea6c72ebb92a237fa2bdf26c5ff16592efcdae.incomplete\ndownloading https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/resolve/f82bd3692de0283f4a4b31e06d164dd8467fb52e/.gitattributes to /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B-Chat/blobs/a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete\ndownloading https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/resolve/f82bd3692de0283f4a4b31e06d164dd8467fb52e/merges.txt to /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B-Chat/blobs/20024bfe7c83998e9aeaf98a0cd6a2ce6306c2f0.incomplete\ndownloading https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/resolve/f82bd3692de0283f4a4b31e06d164dd8467fb52e/LICENSE to /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B-Chat/blobs/c5faab3b0ab37a1f8fe7067d963bf30e8735d7e1.incomplete\ndownloading https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/resolve/f82bd3692de0283f4a4b31e06d164dd8467fb52e/generation_config.json to /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B-Chat/blobs/4ec054254ec0f33b475f9e84d400782704a95014.incomplete\ndownloading https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/resolve/f82bd3692de0283f4a4b31e06d164dd8467fb52e/model.safetensors to /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B-Chat/blobs/72453a8ccb338811935ab95a3a6ffa86b586807bf5b3dc327f28b5389b5636e6.incomplete\n\nconfig.json: 100%|█████████████████████████████| 661/661 [00:00<00:00, 5.51MB/s]\u001b[A\n\ntokenizer.json:   0%|                               | 0.00/7.03M [00:00<?, ?B/s]\u001b[A\n\nmerges.txt:   0%|                                   | 0.00/1.67M [00:00<?, ?B/s]\u001b[A\u001b[A\n\n\ngeneration_config.json: 100%|██████████████████| 206/206 [00:00<00:00, 2.09MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nLICENSE: 100%|█████████████████████████████| 7.28k/7.28k [00:00<00:00, 22.5MB/s]\u001b[A\u001b[A\u001b[A\ndownloading https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/resolve/f82bd3692de0283f4a4b31e06d164dd8467fb52e/README.md to /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B-Chat/blobs/1deb39f953b0781faf56d8e597a7355c9f8468f0.incomplete\n\n\n\n.gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 13.3MB/s]\u001b[A\u001b[A\u001b[A\nFetching 10 files:  10%|██▌                      | 1/10 [00:00<00:01,  5.83it/s]downloading https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/resolve/f82bd3692de0283f4a4b31e06d164dd8467fb52e/tokenizer_config.json to /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B-Chat/blobs/b1797ada900fbcebfe7388cefbf28e8e5b0cfe21.incomplete\n\n\n\nmodel.safetensors:   0%|                            | 0.00/1.24G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n\n\n\nREADME.md: 100%|███████████████████████████| 4.28k/4.28k [00:00<00:00, 24.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\ndownloading https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat/resolve/f82bd3692de0283f4a4b31e06d164dd8467fb52e/vocab.json to /root/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B-Chat/blobs/4783fe10ac3adce15ac8f358ef5462739852c569.incomplete\n\n\n\n\ntokenizer_config.json: 100%|███████████████| 1.29k/1.29k [00:00<00:00, 10.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmerges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 20.7MB/s]\n\n\nvocab.json:   0%|                                   | 0.00/2.78M [00:00<?, ?B/s]\u001b[A\u001b[A\n\n\nmodel.safetensors:   1%|▏                  | 10.5M/1.24G [00:00<00:24, 49.3MB/s]\u001b[A\u001b[A\u001b[A\ntokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 22.0MB/s]\u001b[A\n\n\n\nmodel.safetensors:   3%|▍                  | 31.5M/1.24G [00:00<00:13, 91.1MB/s]\u001b[A\u001b[A\u001b[A\n\nvocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 6.41MB/s]\u001b[A\u001b[A\n\n\n\nmodel.safetensors:   5%|█                   | 62.9M/1.24G [00:00<00:08, 139MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:   7%|█▎                  | 83.9M/1.24G [00:00<00:07, 156MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:   8%|█▊                   | 105M/1.24G [00:00<00:06, 167MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  10%|██▏                  | 126M/1.24G [00:00<00:06, 176MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  12%|██▍                  | 147M/1.24G [00:00<00:06, 182MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  14%|██▊                  | 168M/1.24G [00:01<00:05, 187MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  15%|███▏                 | 189M/1.24G [00:01<00:05, 192MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  17%|███▌                 | 210M/1.24G [00:01<00:05, 194MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  19%|███▉                 | 231M/1.24G [00:01<00:05, 195MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  20%|████▎                | 252M/1.24G [00:01<00:05, 195MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  22%|████▌                | 273M/1.24G [00:01<00:04, 196MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  24%|████▉                | 294M/1.24G [00:01<00:04, 196MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  25%|█████▎               | 315M/1.24G [00:01<00:04, 197MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  27%|█████▋               | 336M/1.24G [00:01<00:04, 198MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  29%|██████               | 357M/1.24G [00:02<00:04, 195MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  30%|██████▍              | 377M/1.24G [00:02<00:04, 197MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  32%|██████▊              | 398M/1.24G [00:02<00:04, 198MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  34%|███████              | 419M/1.24G [00:02<00:04, 200MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  36%|███████▍             | 440M/1.24G [00:02<00:03, 203MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  37%|███████▊             | 461M/1.24G [00:02<00:03, 204MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  39%|████████▏            | 482M/1.24G [00:02<00:03, 204MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  41%|████████▋            | 514M/1.24G [00:02<00:03, 207MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  43%|█████████            | 535M/1.24G [00:02<00:03, 205MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  46%|█████████▌           | 566M/1.24G [00:03<00:03, 207MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  47%|█████████▉           | 587M/1.24G [00:03<00:03, 207MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  49%|██████████▎          | 608M/1.24G [00:03<00:03, 208MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  51%|██████████▋          | 629M/1.24G [00:03<00:02, 207MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  53%|███████████▏         | 661M/1.24G [00:03<00:02, 209MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  56%|███████████▋         | 692M/1.24G [00:03<00:02, 210MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  58%|████████████         | 713M/1.24G [00:03<00:02, 209MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  60%|████████████▌        | 744M/1.24G [00:03<00:02, 209MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  62%|████████████▉        | 765M/1.24G [00:03<00:02, 207MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  63%|█████████████▎       | 786M/1.24G [00:04<00:02, 207MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  65%|█████████████▋       | 807M/1.24G [00:04<00:02, 207MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  68%|██████████████▏      | 839M/1.24G [00:04<00:01, 209MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  70%|██████████████▋      | 870M/1.24G [00:04<00:01, 210MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  73%|███████████████▎     | 902M/1.24G [00:04<00:01, 210MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  74%|███████████████▋     | 923M/1.24G [00:04<00:01, 208MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  76%|███████████████▉     | 944M/1.24G [00:04<00:01, 207MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  79%|████████████████▌    | 975M/1.24G [00:04<00:01, 209MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  81%|████████████████▏   | 1.01G/1.24G [00:05<00:01, 212MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  84%|████████████████▊   | 1.04G/1.24G [00:05<00:00, 212MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  86%|█████████████████▎  | 1.07G/1.24G [00:05<00:00, 212MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  89%|█████████████████▊  | 1.10G/1.24G [00:05<00:00, 205MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  91%|██████████████████  | 1.12G/1.24G [00:05<00:00, 204MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  93%|██████████████████▌ | 1.15G/1.24G [00:05<00:00, 206MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  95%|██████████████████▉ | 1.17G/1.24G [00:05<00:00, 206MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  96%|███████████████████▎| 1.20G/1.24G [00:06<00:00, 206MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors:  98%|███████████████████▋| 1.22G/1.24G [00:06<00:00, 203MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel.safetensors: 100%|████████████████████| 1.24G/1.24G [00:06<00:00, 197MB/s]\u001b[A\u001b[A\u001b[A\nFetching 10 files: 100%|████████████████████████| 10/10 [00:06<00:00,  1.54it/s]\n/kaggle/working/Qwen\n# Copyright (c) OpenMMLab. All rights reserved.\nfrom datasets import load_dataset\nfrom mmengine.dataset import DefaultSampler\nfrom mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,\n                            LoggerHook, ParamSchedulerHook)\nfrom mmengine.optim import AmpOptimWrapper, CosineAnnealingLR, LinearLR\nfrom torch.optim import AdamW\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom xtuner.dataset import process_hf_dataset\nfrom xtuner.dataset.collate_fns import default_collate_fn\nfrom xtuner.dataset.map_fns import alpaca_map_fn, template_map_fn_factory\nfrom xtuner.engine.hooks import (DatasetInfoHook, EvaluateChatHook,\n                                 VarlenAttnArgsToMessageHubHook)\nfrom xtuner.engine.runner import TrainLoop\nfrom xtuner.model import SupervisedFinetune\nfrom xtuner.utils import PROMPT_TEMPLATE, SYSTEM_TEMPLATE\n\n#######################################################################\n#                          PART 1  Settings                           #\n#######################################################################\n# Model\npretrained_model_name_or_path = \"/kaggle/working/Qwen\"\nuse_varlen_attn = False\n\n# Data\ndata_path = \"/kaggle/working/EmoLLM/datasets/data_pro.json\"\nprompt_template = PROMPT_TEMPLATE.qwen_chat\nmax_length = 2048\npack_to_max_length = True\n\n# Scheduler & Optimizer\nbatch_size = 1  # per_device\naccumulative_counts = 64\ndataloader_num_workers = 0\nmax_epochs = 3\noptim_type = AdamW\nlr = 2e-5\nbetas = (0.9, 0.999)\nweight_decay = 0\nmax_norm = 1  # grad clip\nwarmup_ratio = 0.03\n\n# Save\nsave_steps = 100\nsave_total_limit = 2  # Maximum checkpoints to keep (-1 means unlimited)\n\n# Evaluate the generation performance during the training\nevaluation_freq = 100\nSYSTEM = \"现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。\"\nevaluation_inputs = [\n    '我压力很大', '生活没意思', \"非常容易羡慕别人啊\"\n]\n\n#######################################################################\n#                      PART 2  Model & Tokenizer                      #\n#######################################################################\ntokenizer = dict(\n    type=AutoTokenizer.from_pretrained,\n    pretrained_model_name_or_path=pretrained_model_name_or_path,\n    trust_remote_code=True,\n    padding_side='right')\n\nmodel = dict(\n    type=SupervisedFinetune,\n    use_varlen_attn=use_varlen_attn,\n    llm=dict(\n        type=AutoModelForCausalLM.from_pretrained,\n        pretrained_model_name_or_path=pretrained_model_name_or_path,\n        trust_remote_code=True))\n\n#######################################################################\n#                      PART 3  Dataset & Dataloader                   #\n#######################################################################\nalpaca_en = dict(\n    type=process_hf_dataset,\n    dataset=dict(type=load_dataset, path='json', data_files=dict(train=data_path)),\n    tokenizer=tokenizer,\n    max_length=max_length,\n    dataset_map_fn=None,\n    template_map_fn=dict(\n        type=template_map_fn_factory, template=prompt_template),\n    remove_unused_columns=True,\n    shuffle_before_pack=True,\n    pack_to_max_length=pack_to_max_length,\n    use_varlen_attn=use_varlen_attn)\n\ntrain_dataloader = dict(\n    batch_size=batch_size,\n    num_workers=dataloader_num_workers,\n    dataset=alpaca_en,\n    sampler=dict(type=DefaultSampler, shuffle=True),\n    collate_fn=dict(type=default_collate_fn, use_varlen_attn=use_varlen_attn))\n\n#######################################################################\n#                    PART 4  Scheduler & Optimizer                    #\n#######################################################################\n# optimizer\noptim_wrapper = dict(\n    type=AmpOptimWrapper,\n    optimizer=dict(\n        type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),\n    clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),\n    accumulative_counts=accumulative_counts,\n    loss_scale='dynamic',\n    dtype='float16')\n\n# learning policy\n# More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501\nparam_scheduler = [\n    dict(\n        type=LinearLR,\n        start_factor=1e-5,\n        by_epoch=True,\n        begin=0,\n        end=warmup_ratio * max_epochs,\n        convert_to_iter_based=True),\n    dict(\n        type=CosineAnnealingLR,\n        eta_min=0.0,\n        by_epoch=True,\n        begin=warmup_ratio * max_epochs,\n        end=max_epochs,\n        convert_to_iter_based=True)\n]\n\n# train, val, test setting\ntrain_cfg = dict(type=TrainLoop, max_epochs=max_epochs)\n\n#######################################################################\n#                           PART 5  Runtime                           #\n#######################################################################\n# Log the dialogue periodically during the training process, optional\ncustom_hooks = [\n    dict(type=DatasetInfoHook, tokenizer=tokenizer),\n    dict(\n        type=EvaluateChatHook,\n        tokenizer=tokenizer,\n        every_n_iters=evaluation_freq,\n        evaluation_inputs=evaluation_inputs,\n        system=SYSTEM,\n        prompt_template=prompt_template)\n]\n\nif use_varlen_attn:\n    custom_hooks += [dict(type=VarlenAttnArgsToMessageHubHook)]\n\n# configure default hooks\ndefault_hooks = dict(\n    # record the time of every iteration.\n    timer=dict(type=IterTimerHook),\n    # print log every 10 iterations.\n    logger=dict(type=LoggerHook, log_metric_by_epoch=False, interval=10),\n    # enable the parameter scheduler.\n    param_scheduler=dict(type=ParamSchedulerHook),\n    # save checkpoint per `save_steps`.\n    checkpoint=dict(\n        type=CheckpointHook,\n        by_epoch=False,\n        interval=save_steps,\n        max_keep_ckpts=save_total_limit),\n    # set sampler seed in distributed evrionment.\n    sampler_seed=dict(type=DistSamplerSeedHook),\n)\n\n# configure environment\nenv_cfg = dict(\n    # whether to enable cudnn benchmark\n    cudnn_benchmark=False,\n    # set multi process parameters\n    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n    # set distributed parameters\n    dist_cfg=dict(backend='nccl'),\n)\n\n# set visualizer\nvisualizer = None\n\n# set log level\nlog_level = 'INFO'\n\n# load from which checkpoint\nload_from = None\n\n# whether to resume training from the loaded checkpoint\nresume = False\n\n# Defaults to use random seed and disable `deterministic`\nrandomness = dict(seed=None, deterministic=False)\n\n# set log processor\nlog_processor = dict(by_epoch=False)\n","output_type":"stream"}]},{"cell_type":"code","source":"!NPROC_PER_NODE=2 xtuner train /kaggle/working/EmoLLM/xtuner_config/qwen1_5_0_5_B_full.py --deepspeed deepspeed_zero2","metadata":{"execution":{"iopub.status.busy":"2024-04-13T10:52:02.309501Z","iopub.execute_input":"2024-04-13T10:52:02.309873Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"04/13 10:52:05 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Use random port: 21143\n[2024-04-13 10:52:07,241] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2024-04-13 10:52:11,559] torch.distributed.run: [WARNING] \n[2024-04-13 10:52:11,559] torch.distributed.run: [WARNING] *****************************************\n[2024-04-13 10:52:11,559] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n[2024-04-13 10:52:11,559] torch.distributed.run: [WARNING] *****************************************\n[2024-04-13 10:52:15,632] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2024-04-13 10:52:15,660] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n  warnings.warn(\n[2024-04-13 10:52:18,066] [INFO] [comm.py:637:init_distributed] cdb=None\n[2024-04-13 10:52:18,067] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n/opt/conda/lib/python3.10/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n  warnings.warn(\n[2024-04-13 10:52:18,238] [INFO] [comm.py:637:init_distributed] cdb=None\n04/13 10:52:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n------------------------------------------------------------\nSystem environment:\n    sys.platform: linux\n    Python: 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\n    CUDA available: True\n    MUSA available: False\n    numpy_random_seed: 67013705\n    GPU 0,1: Tesla T4\n    CUDA_HOME: /usr/local/cuda\n    NVCC: Cuda compilation tools, release 12.1, V12.1.105\n    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n    PyTorch: 2.1.2\n    PyTorch compiling details: PyTorch built with:\n  - GCC 9.4\n  - C++ Version: 201703\n  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications\n  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n  - LAPACK is enabled (usually provided by MKL)\n  - NNPACK is enabled\n  - CPU capability usage: AVX2\n  - CUDA Runtime 12.1\n  - NVCC architecture flags: -gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_70,code=compute_70;-gencode;arch=compute_75,code=compute_75\n  - CuDNN 8.9\n  - Magma 2.6.1\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.0, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n\n    TorchVision: 0.16.2\n    OpenCV: 4.9.0\n    MMEngine: 0.10.3\n\nRuntime environment:\n    launcher: pytorch\n    randomness: {'seed': None, 'deterministic': False}\n    cudnn_benchmark: False\n    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n    dist_cfg: {'backend': 'nccl'}\n    seed: None\n    deterministic: False\n    Distributed launcher: pytorch\n    Distributed training: True\n    GPU number: 2\n------------------------------------------------------------\n\n04/13 10:52:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\nSYSTEM = '现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。'\naccumulative_counts = 64\nalpaca_en = dict(\n    dataset=dict(\n        data_files=dict(train='/kaggle/working/EmoLLM/datasets/data_pro.json'),\n        path='json',\n        type='datasets.load_dataset'),\n    dataset_map_fn=None,\n    max_length=2048,\n    pack_to_max_length=True,\n    remove_unused_columns=True,\n    shuffle_before_pack=True,\n    template_map_fn=dict(\n        template='xtuner.utils.PROMPT_TEMPLATE.qwen_chat',\n        type='xtuner.dataset.map_fns.template_map_fn_factory'),\n    tokenizer=dict(\n        padding_side='right',\n        pretrained_model_name_or_path='/kaggle/working/Qwen',\n        trust_remote_code=True,\n        type='transformers.AutoTokenizer.from_pretrained'),\n    type='xtuner.dataset.process_hf_dataset',\n    use_varlen_attn=False)\nbatch_size = 1\nbetas = (\n    0.9,\n    0.999,\n)\ncustom_hooks = [\n    dict(\n        tokenizer=dict(\n            padding_side='right',\n            pretrained_model_name_or_path='/kaggle/working/Qwen',\n            trust_remote_code=True,\n            type='transformers.AutoTokenizer.from_pretrained'),\n        type='xtuner.engine.hooks.DatasetInfoHook'),\n    dict(\n        evaluation_inputs=[\n            '我压力很大',\n            '生活没意思',\n            '非常容易羡慕别人啊',\n        ],\n        every_n_iters=100,\n        prompt_template='xtuner.utils.PROMPT_TEMPLATE.qwen_chat',\n        system='现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。',\n        tokenizer=dict(\n            padding_side='right',\n            pretrained_model_name_or_path='/kaggle/working/Qwen',\n            trust_remote_code=True,\n            type='transformers.AutoTokenizer.from_pretrained'),\n        type='xtuner.engine.hooks.EvaluateChatHook'),\n]\ndata_path = '/kaggle/working/EmoLLM/datasets/data_pro.json'\ndataloader_num_workers = 0\ndefault_hooks = dict(\n    checkpoint=dict(\n        by_epoch=False,\n        interval=100,\n        max_keep_ckpts=2,\n        type='mmengine.hooks.CheckpointHook'),\n    logger=dict(\n        interval=10,\n        log_metric_by_epoch=False,\n        type='mmengine.hooks.LoggerHook'),\n    param_scheduler=dict(type='mmengine.hooks.ParamSchedulerHook'),\n    sampler_seed=dict(type='mmengine.hooks.DistSamplerSeedHook'),\n    timer=dict(type='mmengine.hooks.IterTimerHook'))\nenv_cfg = dict(\n    cudnn_benchmark=False,\n    dist_cfg=dict(backend='nccl'),\n    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\nevaluation_freq = 100\nevaluation_inputs = [\n    '我压力很大',\n    '生活没意思',\n    '非常容易羡慕别人啊',\n]\nlauncher = 'pytorch'\nload_from = None\nlog_level = 'INFO'\nlog_processor = dict(by_epoch=False)\nlr = 2e-05\nmax_epochs = 3\nmax_length = 2048\nmax_norm = 1\nmodel = dict(\n    llm=dict(\n        pretrained_model_name_or_path='/kaggle/working/Qwen',\n        trust_remote_code=True,\n        type='transformers.AutoModelForCausalLM.from_pretrained'),\n    type='xtuner.model.SupervisedFinetune',\n    use_varlen_attn=False)\noptim_type = 'torch.optim.AdamW'\noptim_wrapper = dict(\n    optimizer=dict(\n        betas=(\n            0.9,\n            0.999,\n        ),\n        lr=2e-05,\n        type='torch.optim.AdamW',\n        weight_decay=0),\n    type='DeepSpeedOptimWrapper')\npack_to_max_length = True\nparam_scheduler = [\n    dict(\n        begin=0,\n        by_epoch=True,\n        convert_to_iter_based=True,\n        end=0.09,\n        start_factor=1e-05,\n        type='mmengine.optim.LinearLR'),\n    dict(\n        begin=0.09,\n        by_epoch=True,\n        convert_to_iter_based=True,\n        end=3,\n        eta_min=0.0,\n        type='mmengine.optim.CosineAnnealingLR'),\n]\npretrained_model_name_or_path = '/kaggle/working/Qwen'\nprompt_template = 'xtuner.utils.PROMPT_TEMPLATE.qwen_chat'\nrandomness = dict(deterministic=False, seed=None)\nresume = False\nrunner_type = 'FlexibleRunner'\nsave_steps = 100\nsave_total_limit = 2\nstrategy = dict(\n    config=dict(\n        bf16=dict(enabled=False),\n        fp16=dict(enabled=True, initial_scale_power=16),\n        gradient_accumulation_steps='auto',\n        gradient_clipping='auto',\n        train_micro_batch_size_per_gpu='auto',\n        zero_allow_untested_optimizer=True,\n        zero_force_ds_cpu_optimizer=False,\n        zero_optimization=dict(overlap_comm=True, stage=2)),\n    exclude_frozen_parameters=True,\n    gradient_accumulation_steps=64,\n    gradient_clipping=1,\n    sequence_parallel_size=1,\n    train_micro_batch_size_per_gpu=1,\n    type='xtuner.engine.DeepSpeedStrategy')\ntokenizer = dict(\n    padding_side='right',\n    pretrained_model_name_or_path='/kaggle/working/Qwen',\n    trust_remote_code=True,\n    type='transformers.AutoTokenizer.from_pretrained')\ntrain_cfg = dict(max_epochs=3, type='xtuner.engine.runner.TrainLoop')\ntrain_dataloader = dict(\n    batch_size=1,\n    collate_fn=dict(\n        type='xtuner.dataset.collate_fns.default_collate_fn',\n        use_varlen_attn=False),\n    dataset=dict(\n        dataset=dict(\n            data_files=dict(\n                train='/kaggle/working/EmoLLM/datasets/data_pro.json'),\n            path='json',\n            type='datasets.load_dataset'),\n        dataset_map_fn=None,\n        max_length=2048,\n        pack_to_max_length=True,\n        remove_unused_columns=True,\n        shuffle_before_pack=True,\n        template_map_fn=dict(\n            template='xtuner.utils.PROMPT_TEMPLATE.qwen_chat',\n            type='xtuner.dataset.map_fns.template_map_fn_factory'),\n        tokenizer=dict(\n            padding_side='right',\n            pretrained_model_name_or_path='/kaggle/working/Qwen',\n            trust_remote_code=True,\n            type='transformers.AutoTokenizer.from_pretrained'),\n        type='xtuner.dataset.process_hf_dataset',\n        use_varlen_attn=False),\n    num_workers=0,\n    sampler=dict(shuffle=True, type='mmengine.dataset.DefaultSampler'))\nuse_varlen_attn = False\nvisualizer = None\nwarmup_ratio = 0.03\nweight_decay = 0\nwork_dir = './work_dirs/qwen1_5_0_5_B_full'\n\n04/13 10:52:21 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmengine\" in the \"builder\" registry tree. As a workaround, the current \"builder\" registry in \"xtuner\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmengine\" is a correct scope, or whether the registry is initialized.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n04/13 10:52:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\nbefore_run:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(BELOW_NORMAL) LoggerHook                         \n -------------------- \nbefore_train:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) IterTimerHook                      \n(NORMAL      ) DatasetInfoHook                    \n(LOW         ) EvaluateChatHook                   \n(VERY_LOW    ) CheckpointHook                     \n -------------------- \nbefore_train_epoch:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) IterTimerHook                      \n(NORMAL      ) DistSamplerSeedHook                \n -------------------- \nbefore_train_iter:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) IterTimerHook                      \n -------------------- \nafter_train_iter:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) IterTimerHook                      \n(BELOW_NORMAL) LoggerHook                         \n(LOW         ) ParamSchedulerHook                 \n(LOW         ) EvaluateChatHook                   \n(VERY_LOW    ) CheckpointHook                     \n -------------------- \nafter_train_epoch:\n(NORMAL      ) IterTimerHook                      \n(LOW         ) ParamSchedulerHook                 \n(VERY_LOW    ) CheckpointHook                     \n -------------------- \nbefore_val:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) DatasetInfoHook                    \n -------------------- \nbefore_val_epoch:\n(NORMAL      ) IterTimerHook                      \n -------------------- \nbefore_val_iter:\n(NORMAL      ) IterTimerHook                      \n -------------------- \nafter_val_iter:\n(NORMAL      ) IterTimerHook                      \n(BELOW_NORMAL) LoggerHook                         \n -------------------- \nafter_val_epoch:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) IterTimerHook                      \n(BELOW_NORMAL) LoggerHook                         \n(LOW         ) ParamSchedulerHook                 \n(VERY_LOW    ) CheckpointHook                     \n -------------------- \nafter_val:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(LOW         ) EvaluateChatHook                   \n -------------------- \nafter_train:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(LOW         ) EvaluateChatHook                   \n(VERY_LOW    ) CheckpointHook                     \n -------------------- \nbefore_test:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) DatasetInfoHook                    \n -------------------- \nbefore_test_epoch:\n(NORMAL      ) IterTimerHook                      \n -------------------- \nbefore_test_iter:\n(NORMAL      ) IterTimerHook                      \n -------------------- \nafter_test_iter:\n(NORMAL      ) IterTimerHook                      \n(BELOW_NORMAL) LoggerHook                         \n -------------------- \nafter_test_epoch:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) IterTimerHook                      \n(BELOW_NORMAL) LoggerHook                         \n -------------------- \nafter_test:\n(VERY_HIGH   ) RuntimeInfoHook                    \n -------------------- \nafter_run:\n(BELOW_NORMAL) LoggerHook                         \n -------------------- \n04/13 10:52:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - xtuner_dataset_timeout = 0:30:00\nMap (num_proc=32): 100%|███████████| 6281/6281 [00:01<00:00, 5670.20 examples/s]\nFilter (num_proc=32): 100%|████████| 6281/6281 [00:00<00:00, 8718.62 examples/s]\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nMap (num_proc=32): 100%|████████████| 6281/6281 [00:14<00:00, 431.10 examples/s]\nFilter (num_proc=32): 100%|████████| 6281/6281 [00:02<00:00, 3138.48 examples/s]\nFlattening the indices (num_proc=32): 100%|█| 6281/6281 [00:00<00:00, 9941.35 ex\nMap (num_proc=32): 100%|███████████| 6281/6281 [00:02<00:00, 2433.13 examples/s]\nMap (num_proc=32): 100%|████████████| 1347/1347 [00:01<00:00, 893.25 examples/s]\n04/13 10:52:50 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Dataset Dataset has no metainfo. ``dataset_meta`` in visualizer will be None.\n[2024-04-13 10:52:54,777] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n[2024-04-13 10:52:56,596] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n[2024-04-13 10:52:56,598] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n[2024-04-13 10:52:56,598] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n[2024-04-13 10:52:56,618] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n[2024-04-13 10:52:56,618] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n[2024-04-13 10:52:56,618] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n[2024-04-13 10:52:56,618] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500,000,000\n[2024-04-13 10:52:56,618] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500,000,000\n[2024-04-13 10:52:56,618] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n[2024-04-13 10:52:56,618] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n[2024-04-13 10:52:58,583] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n[2024-04-13 10:52:58,584] [INFO] [utils.py:801:see_memory_usage] MA 1.92 GB         Max_MA 2.35 GB         CA 3.08 GB         Max_CA 3 GB \n[2024-04-13 10:52:58,584] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 6.07 GB, percent = 19.4%\n[2024-04-13 10:52:58,761] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n[2024-04-13 10:52:58,762] [INFO] [utils.py:801:see_memory_usage] MA 1.92 GB         Max_MA 2.78 GB         CA 3.94 GB         Max_CA 4 GB \n[2024-04-13 10:52:58,762] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 6.07 GB, percent = 19.4%\n[2024-04-13 10:52:58,763] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n[2024-04-13 10:52:58,942] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n[2024-04-13 10:52:58,943] [INFO] [utils.py:801:see_memory_usage] MA 1.92 GB         Max_MA 1.92 GB         CA 3.94 GB         Max_CA 4 GB \n[2024-04-13 10:52:58,943] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 6.07 GB, percent = 19.4%\n[2024-04-13 10:52:58,946] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n[2024-04-13 10:52:58,946] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n[2024-04-13 10:52:58,946] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n[2024-04-13 10:52:58,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[(0.9, 0.999)]\n[2024-04-13 10:52:58,947] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n[2024-04-13 10:52:58,947] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n    \"partition_activations\": false, \n    \"contiguous_memory_optimization\": false, \n    \"cpu_checkpointing\": false, \n    \"number_checkpoints\": null, \n    \"synchronize_checkpoint_boundary\": false, \n    \"profile\": false\n}\n[2024-04-13 10:52:58,948] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n[2024-04-13 10:52:58,948] [INFO] [config.py:1000:print]   amp_enabled .................. False\n[2024-04-13 10:52:58,948] [INFO] [config.py:1000:print]   amp_params ................... False\n[2024-04-13 10:52:58,948] [INFO] [config.py:1000:print]   autotuning_config ............ {\n    \"enabled\": false, \n    \"start_step\": null, \n    \"end_step\": null, \n    \"metric_path\": null, \n    \"arg_mappings\": null, \n    \"metric\": \"throughput\", \n    \"model_info\": null, \n    \"results_dir\": \"autotuning_results\", \n    \"exps_dir\": \"autotuning_exps\", \n    \"overwrite\": true, \n    \"fast\": true, \n    \"start_profile_step\": 3, \n    \"end_profile_step\": 5, \n    \"tuner_type\": \"gridsearch\", \n    \"tuner_early_stopping\": 5, \n    \"tuner_num_trials\": 50, \n    \"model_info_path\": null, \n    \"mp_size\": 1, \n    \"max_train_batch_size\": null, \n    \"min_train_batch_size\": 1, \n    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n    \"min_train_micro_batch_size_per_gpu\": 1, \n    \"num_tuning_micro_batch_sizes\": 3\n}\n[2024-04-13 10:52:58,948] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False\n[2024-04-13 10:52:58,948] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n[2024-04-13 10:52:58,948] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n[2024-04-13 10:52:58,948] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n[2024-04-13 10:52:58,948] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n[2024-04-13 10:52:58,948] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7d1260621810>\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   communication_data_type ...... None\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   disable_allgather ............ False\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   dump_state ................... False\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n[2024-04-13 10:52:58,949] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n    \"enabled\": false, \n    \"recompute_fwd_factor\": 0.0, \n    \"profile_step\": 1, \n    \"module_depth\": -1, \n    \"top_modules\": 1, \n    \"detailed\": true, \n    \"output_file\": null\n}\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   fp16_auto_cast ............... False\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   fp16_enabled ................. True\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   global_rank .................. 0\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 64\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   gradient_clipping ............ 1\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   loss_scale ................... 0\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n[2024-04-13 10:52:58,950] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   nebula_config ................ {\n    \"enabled\": false, \n    \"persistent_storage_path\": null, \n    \"persistent_time_interval\": 100, \n    \"num_of_version_in_retention\": 2, \n    \"enable_nebula_load\": true, \n    \"load_path\": null\n}\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   optimizer_name ............... None\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   optimizer_params ............. None\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   pld_enabled .................. False\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   pld_params ................... False\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   scheduler_name ............... None\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   scheduler_params ............. None\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   sparse_attention ............. None\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   steps_per_print .............. 10000000000000\n[2024-04-13 10:52:58,951] [INFO] [config.py:1000:print]   train_batch_size ............. 128\n[2024-04-13 10:52:58,952] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1\n[2024-04-13 10:52:58,952] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n[2024-04-13 10:52:58,952] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n[2024-04-13 10:52:58,952] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n[2024-04-13 10:52:58,952] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n[2024-04-13 10:52:58,952] [INFO] [config.py:1000:print]   world_size ................... 2\n[2024-04-13 10:52:58,952] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n[2024-04-13 10:52:58,952] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n[2024-04-13 10:52:58,952] [INFO] [config.py:1000:print]   zero_enabled ................. True\n[2024-04-13 10:52:58,952] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. False\n[2024-04-13 10:52:58,952] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n[2024-04-13 10:52:58,952] [INFO] [config.py:986:print_user_config]   json = {\n    \"gradient_accumulation_steps\": 64, \n    \"train_micro_batch_size_per_gpu\": 1, \n    \"gradient_clipping\": 1, \n    \"zero_allow_untested_optimizer\": true, \n    \"zero_force_ds_cpu_optimizer\": false, \n    \"zero_optimization\": {\n        \"stage\": 2, \n        \"overlap_comm\": true\n    }, \n    \"fp16\": {\n        \"enabled\": true, \n        \"initial_scale_power\": 16\n    }, \n    \"bf16\": {\n        \"enabled\": false\n    }, \n    \"steps_per_print\": 1.000000e+13\n}\n04/13 10:52:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Num train samples 1347\n04/13 10:52:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - train example:\n2024-04-13 10:52:59.663881: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-13 10:52:59.663905: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-13 10:52:59.663960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-13 10:52:59.664063: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-13 10:52:59.773856: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-04-13 10:52:59.773851: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n04/13 10:53:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - <|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n医生，我最近在工作上感到特别焦虑，每次看到邮箱里的新邮件，我的心都会跳到嗓子眼，我害怕打开它们，因为里面总会有新的任务或者批评。\n\n<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，工作压力确实可以让人感到焦虑。我们先从简单的问题开始，你能告诉我，你最担心邮件里会出现什么类型的信息？\n\n<|im_end|>\n<|im_start|>user\n我总是担心老板会不满意我的工作，或者同事会提出一些我无法解决的问题。我感觉自己总是在追赶进度，但永远也追不上。\n\n<|im_end|>\n<|im_start|>assistant\n听起来你的焦虑可能源于对工作表现的担忧和自我施加的压力。这种情况很常见。你能分享一个最近发生的让你感到焦虑的具体工作场景吗？\n\n<|im_end|>\n<|im_start|>user\n上个星期，我负责的一个项目临近截止日期，我连续加班好几天，但最后还是因为一些细节没处理好被老板批评了。我感到非常沮丧，觉得自己不够好。\n\n<|im_end|>\n<|im_start|>assistant\n面对这样的情况，很多人都会有你这样的感受。首先，我想告诉你，你已经做得很好了，能够连续加班并且完成大部分工作。我们每个人都会有不足之处，关键是学会接受并从中学习。你能告诉我，你觉得是什么原因导致了那些细节上的失误吗？\n\n<|im_end|>\n<|im_start|>user\n我觉得可能是因为我太急于求成了，没有合理安排时间，导致最后匆忙中出现了疏漏。\n\n<|im_end|>\n<|im_start|>assistant\n这是一个非常重要的洞察。有时候，我们确实需要在工作中放慢脚步，合理规划时间，这样可以提高工作效率和完成质量。我建议你可以尝试制定一个工作计划，将大任务分解为小步骤，并为每个步骤设置一个可实现的时间目标。\n\n<|im_end|>\n<|im_start|>user\n听起来不错，但我觉得自己有时候很难坚持这样的计划。\n\n<|im_end|>\n<|im_start|>assistant\n这很正常。改变习惯需要时间和耐心。你可以从小事做起，比如每天列出第二天必须完成的三个小目标。完成后给自己一些正面的反馈，这样可以帮助你建立起信心和动力。另外，如果遇到困难，不妨寻求同事的帮助，团队协作也是缓解压力的一个好方法。\n\n<|im_end|>\n<|im_start|>user\n嗯，我试试看。但是，如果我又感到焦虑了，该怎么办呢？\n\n<|im_end|>\n<|im_start|>assistant\n当你感到焦虑时，可以尝试一些放松技巧，比如深呼吸、短暂散步或冥想。这些方法能够帮助你从紧张的状态中抽离出来，回归到更加平静和理性的思考。同时，记得随时提醒自己，焦虑是一种情绪反应，它并不能决定你的工作表现。\n\n（结束语）\n<|im_end|>\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n医生，我最近总是感到很沮丧，对学业提不起任何兴趣。我觉得自己无论怎么努力都达不到预期的目标，真的很痛苦。<|im_end|>\n<|im_start|>assistant\n我明白你的感受，这种情况在面临较大学业压力时十分常见。首先，请不要对自己过于苛责，每个人都有遇到困难的时候。尝试调整一下心态，将大目标分解成小目标，每达成一个小目标就奖励自己，这样可能会帮你找回学习的乐趣和动力。同时，你是否可以具体描述一下你近期面临的学业挑战和担忧？<|im_end|>\n<|im_start|>user\n我最近的课程难度提升了很多，尤其是一门专业课，我感觉自己怎么学都理解不透彻，考试也总是失败。这让我觉得自己很笨，可能不适合这个专业。<|im_end|>\n<|im_start|>assistant\n明白了，看来你在特定课程上的挫败感是导致你情绪低落的主要原因。要知道，学术能力并不能衡量一个人的整体价值，而且学习新知识的过程中遇到瓶颈是正常的。你可以尝试换种学习方法，比如寻找相关资料辅助理解、参加学习小组讨论或者寻求教师的一对一指导。同时，保持耐心，给自己足够的时间去适应和掌握新的知识点。另外，如果你持续对此有强烈的自我否定感，我们可以考虑进行一些认知行为疗法来帮助你重塑自我认知，合理看待学业挑战。<|im_end|>\n<|im_start|>user\n我确实试过很多方法，但还是觉得无法突破，甚至有时候会失眠，害怕面对第二天的学习。<|im_end|>\n<|im_start|>assistant\n你的困扰我非常重视。失眠可能是焦虑情绪的一种表现，建议你在睡前做一些放松活动，如阅读、深呼吸练习或轻度瑜伽等以助于改善睡眠质量。此外，针对你对学业的恐惧感，我们可以在日程安排上做出调整，合理规划每日学习任务，避免一次性承受过多压力。如果条件允许，学校的心理咨询服务也是个很好的资源，可以提供更专业的心理支持。最后，若以上策略实施后仍无明显改善，我们再探讨是否需要进一步的心理治疗或者药物干预。记住，你并不孤单，我们会一起找出最适合你的解决方案。<|im_end|>\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n医生，我最近总是感到很焦虑，晚上也睡不着觉，这种情况已经持续一段时间了。\n\n<|im_end|>\n<|im_start|>assistant\n我能理解你现在的感受，焦虑和失眠确实让人很难受。首先，你能告诉我是什么让你感到焦虑吗？或者有没有什么特定的事情发生在这段时间？\n\n<|im_end|>\n<|im_start|>user\n其实我也说不清楚，就是觉得生活压力很大，工作、家庭各方面都不太顺心。有时候觉得自己很无助，对未来也很迷茫。\n\n<|im_end|>\n<|im_start|>assistant\n听起来你承受了很大的压力。面对生活中的困境，感到无助和迷茫是很正常的。我们可以尝试一步步来解决这个问题。首先，你能告诉我，在目前的情况下，你觉得有哪些问题是你可以着手去解决的呢？\n\n<|im_end|>\n<|im_start|>user\n我觉得可能需要调整一下工作状态，但不知道从何入手。另外，家庭方面也有一些矛盾，让我觉得很疲惫。\n\n<|im_end|>\n<|im_start|>assistant\n那我们先从工作方面来说，你觉得工作中哪些方面让你感到压力最大？我们可以尝试制定一个计划，逐步去改善这些状况。\n\n<|im_end|>\n<|im_start|>user\n我觉得工作量太大，而且和同事之间的沟通也不是很顺畅。\n\n<|im_end|>\n<|im_start|>assistant\n那么，我们可以尝试从以下几个方面来调整：首先，合理安排工作，分清主次，不要给自己太大的压力；其次，加强与同事之间的沟通，了解彼此的需求和困难，共同解决问题。此外，你还可以尝试一些放松的方法，比如深呼吸、瑜伽等，帮助自己缓解焦虑。\n\n<|im_end|>\n<|im_start|>user\n那家庭方面的问题该怎么办呢？\n\n<|im_end|>\n<|im_start|>assistant\n家庭问题我们可以采取同样的方法，先找出问题的根源，然后和家人一起沟通，寻求解决方案。在这个过程中，保持冷静和耐心非常重要。另外，你可以尝试和家人一起参加一些活动，增进彼此的感情。\n\n<|im_end|>\n<|im_start|>user\n谢谢医生，听了你的建议，我觉得心情好多了。我会按照你说的去尝试调整。\n\n<|im_end|>\n<|im_start|>assistant\n不用谢，很高兴能帮到你。记住，面对问题要勇敢地去面对，不要把压力都放在自己身上。如果以后还有任何困扰，随时都可以来找我。祝你早日走出困境，生活愉快！<|im_end|>\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n医生，我最近感觉特别兴奋，有时候甚至控制不住自己的情绪，有点担心自己是不是出了什么问题？\n\n<|im_end|>\n<|im_start|>assistant\n我能理解你的担忧。兴奋是一种正常的情绪反应，但过度的兴奋可能会对我们的日常生活造成影响。你能告诉我，是什么让你感到如此兴奋吗？我们可以一起探讨一下。\n\n<|im_end|>\n<|im_start|>user\n其实也没什么事，就是最近工作比较顺利，得到了领导的表扬，然后我就觉得自己充满了能量，有时候晚上都睡不着觉。\n\n<|im_end|>\n<|im_start|>assistant\n听起来你的工作表现很出色，这是值得庆祝的事情。不过，过度兴奋导致的失眠确实需要注意。我们可以尝试一些方法来帮助你调整情绪。首先，你可以在睡前尝试一些放松的活动，比如深呼吸、瑜伽或冥想，这有助于你平静下来。\n\n<|im_end|>\n<|im_start|>user\n嗯，我试试看。但是有时候我兴奋得连这些都没法做，怎么办？\n\n<|im_end|>\n<|im_start|>assistant\n在这种情况下，你可以尝试逐步将自己的注意力转移到其他事物上，比如做一些有规律的运动，将过多的精力消耗掉。此外，制定一个合理的作息时间表也很重要，尽量保持每天同一时间上床睡觉和起床。\n\n<|im_end|>\n<|im_start|>user\n那我应该怎么控制自己的情绪，避免过度兴奋呢？\n\n<|im_end|>\n<|im_start|>assistant\n情绪管理是一个逐步的过程。你可以尝试以下几个方法：1. 当你感到兴奋时，及时意识到自己的情绪，并告诉自己要冷静；2. 分析让你兴奋的原因，是否有可能导致不良后果；3. 与他人分享你的喜悦，这样既能保持愉快的心情，又能避免过度兴奋。\n\n<|im_end|>\n<|im_start|>user\n谢谢医生，我会按照你的建议去尝试。但如果我发现自己无法控制情绪，应该怎么办？\n\n<|im_end|>\n<|im_start|>assistant\n如果你发现自己无法独立解决问题，那么寻求专业帮助是非常重要的。你可以定期来咨询我，或者加入一些心理辅导小组，与其他人一起分享和解决问题。记住，面对心理健康问题，勇敢寻求帮助是明智的选择。\n\n<|im_end|>\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n医生，我最近一直感到很害怕，连我自己也不知道为什么会这样。\n\n<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，这是一种常见的情绪体验。我们先尝试放慢节奏，你能告诉我，你是在什么情况下会感到这种恐惧吗？\n\n<|im_end|>\n<|im_start|>user\n嗯，比如说，当我一个人在家的时候，我会觉得很不安全，甚至晚上睡觉都会开着灯。\n\n<|im_end|>\n<|im_start|>assistant\n我明白你的感受，这种不安全感可能会让人感到非常不安。我想问一下，这种恐惧感是从什么时候开始出现的呢？有没有什么特别的事件触发它？\n\n<|im_end|>\n<|im_start|>user\n让我\n04/13 10:53:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - before_train in EvaluateChatHook.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2756: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\n  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n04/13 10:53:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n压力是生活中不可避免的一部分，但是我们可以采取一些措施来减轻压力。首先，你可以尝试进行一些放松的活动，比如深呼吸、冥想、瑜伽等，这些活动可以帮助你放松身心，减轻压力。其次，你可以尝试与\n\n04/13 10:53:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n生活没有意思，可能有很多原因。首先，你可能感到压力过大，工作压力、人际关系压力、财务压力等都可能影响你的生活。其次，你可能感到孤独，没有朋友、家人和共同的兴趣爱好。最后，你\n\n04/13 10:53:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n羡慕别人是一种正常的心理反应，每个人都有自己的优点和特点，我们应该学会欣赏和尊重他人的优点，而不是过度羡慕。同时，我们也可以通过学习和实践来提高自己的能力，从而更好地适应社会。如果你感到羡慕别人，\n\n04/13 10:53:13 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n04/13 10:53:13 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n04/13 10:53:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /kaggle/working/work_dirs/qwen1_5_0_5_B_full.\n04/13 10:53:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [  10/2022]  lr: 3.0510e-06  eta: 0:28:33  time: 0.8515  data_time: 0.0067  memory: 8012  loss: 2.7256\n04/13 10:53:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [  20/2022]  lr: 6.4408e-06  eta: 0:28:22  time: 0.8488  data_time: 0.0065  memory: 8012  loss: 2.5995\n04/13 10:53:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [  30/2022]  lr: 9.8306e-06  eta: 0:28:17  time: 0.8558  data_time: 0.0071  memory: 8012  loss: 2.5710\n04/13 10:53:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [  40/2022]  lr: 1.3220e-05  eta: 0:28:11  time: 0.8576  data_time: 0.0064  memory: 8012  loss: 2.6451\n04/13 10:53:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [  50/2022]  lr: 1.6610e-05  eta: 0:28:06  time: 0.8630  data_time: 0.0065  memory: 8012  loss: 2.5334\n04/13 10:54:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [  60/2022]  lr: 2.0000e-05  eta: 0:28:02  time: 0.8697  data_time: 0.0072  memory: 8012  loss: 2.6467\n[2024-04-13 10:54:08,686] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n04/13 10:54:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [  70/2022]  lr: 1.9999e-05  eta: 0:28:00  time: 0.8789  data_time: 0.0067  memory: 8456  loss: 2.5550\n04/13 10:54:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [  80/2022]  lr: 1.9995e-05  eta: 0:27:55  time: 0.8785  data_time: 0.0065  memory: 8456  loss: 2.6337\n04/13 10:54:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [  90/2022]  lr: 1.9989e-05  eta: 0:27:51  time: 0.8844  data_time: 0.0065  memory: 8456  loss: 2.6976\n04/13 10:54:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 100/2022]  lr: 1.9981e-05  eta: 0:27:47  time: 0.8901  data_time: 0.0066  memory: 8456  loss: 2.5006\n04/13 10:54:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 10:54:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n压力是生活中不可避免的一部分，但是我们可以通过一些方法来减轻压力。首先，你可以尝试进行一些放松的活动，比如深呼吸、冥想、瑜伽等，这些活动可以帮助你放松身心，减轻压力。其次，你可以尝试寻求专业的心理咨询，比如咨询师、心理医生等，他们可以提供专业的建议和治疗，帮助你处理压力。最后，你也可以尝试改变自己的生活方式，比如保持良好的饮食习惯、规律的作息时间、适量的运动等，这些都可以帮助你减轻压力。<|im_end|>\n\n04/13 10:54:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n生活没有意思，可能是因为你对生活没有热情，或者你对当前的生活状态感到不满。以下是一些可能的解决方法：\n\n1. 自我反思：花时间思考你为什么感到生活没有意思，是什么让你感到不满的。这可能需要一些时间和深度的思考。\n\n2. 找到兴趣爱好：找到你真正热爱的事情，这可以帮助你保持生活的热情和动力。\n\n3. 建立良好的人际关系：与朋友、家人和同事建立良好的关系，可以帮助你获得支持和帮助，从而提高生活满意度。\n\n4. 学习新技能：学习新技能可以帮助你提高生活满意度，同时也可以增加你的自信心。\n\n5. 保持积极的心态：保持积极的心态可以帮助你更好地应对生活中的挑战，从而提高生活满意度。\n\n6. 寻求专业帮助：如果你发现自己无法解决生活中的问题，可能需要寻求专业的心理咨询师的帮助。\n\n记住，每个人的生活都是独特的，所以找到最适合自己的解决方法可能需要一些时间和努力。<|im_end|>\n\n04/13 10:54:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n羡慕别人是一种正常的心理反应，每个人都有自己的优点和特点，我们应该学会欣赏和尊重他人的优点，而不是过度羡慕。同时，我们也可以通过学习和实践来提高自己的能力，从而更好地满足自己的需求。同时，我们也可以通过与他人交流和互动，来增强自己的社交能力和人际关系。<|im_end|>\n\n04/13 10:54:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 100 iterations\n[2024-04-13 10:54:51,729] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_100.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 10:54:51,740] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_100.pth/mp_rank_00_model_states.pt\n[2024-04-13 10:54:51,741] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_100.pth/mp_rank_00_model_states.pt...\n[2024-04-13 10:54:53,601] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_100.pth/mp_rank_00_model_states.pt.\n[2024-04-13 10:54:53,602] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_100.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 10:54:53,602] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_100.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 10:54:55,555] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_100.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 10:54:55,555] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_100.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 10:54:55,555] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_100.pth is ready now!\n[2024-04-13 10:54:55,594] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_100.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 10:54:55,594] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_100.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 10:54:55,594] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_100.pth is ready now!\n04/13 10:55:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 110/2022]  lr: 1.9969e-05  eta: 0:32:07  time: 2.4136  data_time: 1.5224  memory: 8456  loss: 2.5248\n04/13 10:55:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 120/2022]  lr: 1.9955e-05  eta: 0:31:40  time: 0.8983  data_time: 0.0068  memory: 8456  loss: 2.3394\n[2024-04-13 10:55:21,051] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n04/13 10:55:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 130/2022]  lr: 1.9939e-05  eta: 0:31:19  time: 0.9270  data_time: 0.0064  memory: 8456  loss: 2.5316\n04/13 10:55:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 140/2022]  lr: 1.9920e-05  eta: 0:30:56  time: 0.8922  data_time: 0.0064  memory: 8456  loss: 2.4619\n04/13 10:55:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 150/2022]  lr: 1.9899e-05  eta: 0:30:34  time: 0.8877  data_time: 0.0066  memory: 8456  loss: 2.4648\n04/13 10:55:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 160/2022]  lr: 1.9875e-05  eta: 0:30:13  time: 0.8855  data_time: 0.0065  memory: 8456  loss: 2.6639\n04/13 10:55:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 170/2022]  lr: 1.9848e-05  eta: 0:29:53  time: 0.8846  data_time: 0.0063  memory: 8456  loss: 2.6427\n04/13 10:56:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 180/2022]  lr: 1.9819e-05  eta: 0:29:35  time: 0.8853  data_time: 0.0068  memory: 8456  loss: 2.4973\n04/13 10:56:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 190/2022]  lr: 1.9787e-05  eta: 0:29:18  time: 0.8863  data_time: 0.0065  memory: 8456  loss: 2.4059\n[2024-04-13 10:56:17,884] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n04/13 10:56:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 200/2022]  lr: 1.9753e-05  eta: 0:29:02  time: 0.8927  data_time: 0.0064  memory: 8456  loss: 2.6568\n04/13 10:56:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 10:56:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n压力是生活中不可避免的一部分，但是我们可以通过一些方法来减轻压力。首先，你可以尝试进行一些放松的活动，比如深呼吸、瑜伽、冥想等，这些活动可以帮助你放松身心，减轻压力。其次，你可以尝试和朋友、家人或者专业的心理咨询师分享你的压力，他们可以提供一些有效的建议和帮助你应对压力。最后，你也可以尝试一些积极的应对压力的方法，比如设定一些小目标，给自己一些奖励，这些都可以帮助你缓解压力。<|im_end|>\n\n04/13 10:56:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n生活没有意思，可能是因为你对生活没有热情，或者你对当前的生活状态感到不满意。以下是一些可能的解决方法：\n\n1. 自我反思：花时间思考你为什么感到生活没有意思，这可以帮助你找到问题的根源。你可以写日记，或者和朋友、家人聊天，分享你的感受。\n\n2. 找到兴趣爱好：如果你对某个领域感兴趣，那么你可能会发现生活中的乐趣。你可以尝试新的事物，或者寻找你以前没有尝试过的事情。\n\n3. 建立目标：设定一些短期和长期的目标，可以帮助你保持动力。你可以设定一些短期的目标，比如每天学习新的技能，或者每周完成一个项目。\n\n4. 保持积极的心态：保持积极的心态可以帮助你更好地应对生活中的挑战。你可以尝试一些积极的思维方式，比如看到生活中的美好，或者看到生活中的困难。\n\n5. 寻求专业帮助：如果你发现自己无法解决生活中的问题，那么你可能需要寻求专业的心理咨询师的帮助。他们可以帮助你找到解决问题的方法，或者提供一些应对压力和焦虑的策略。\n\n记住，每个人的生活都是独特的，所以你可能需要找到最适合你的解决方法。<|im_end|>\n\n04/13 10:56:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n羡慕别人是一种正常的心理反应，每个人都有自己的优点和特点，我们应该学会欣赏和尊重他人的优点，而不是过度羡慕。同时，我们也应该学会控制自己的情绪，不要因为别人的成功而感到骄傲，也不要因为别人的失败而感到沮丧。我们应该学会自我调节，保持良好的心态，这样才能更好地面对生活中的挑战。<|im_end|>\n\n04/13 10:56:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 200 iterations\n[2024-04-13 10:56:37,401] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_200.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 10:56:37,410] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_200.pth/mp_rank_00_model_states.pt\n[2024-04-13 10:56:37,410] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_200.pth/mp_rank_00_model_states.pt...\n[2024-04-13 10:56:39,307] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_200.pth/mp_rank_00_model_states.pt.\n[2024-04-13 10:56:39,309] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_200.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 10:56:39,309] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_200.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 10:56:41,860] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_200.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 10:56:41,860] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_200.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 10:56:41,861] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_200.pth is ready now!\n[2024-04-13 10:56:42,065] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_200.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 10:56:42,100] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_200.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 10:56:42,100] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_200.pth is ready now!\n04/13 10:56:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 210/2022]  lr: 1.9717e-05  eta: 0:31:15  time: 2.6039  data_time: 1.7189  memory: 8456  loss: 2.6508\n04/13 10:56:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 220/2022]  lr: 1.9678e-05  eta: 0:30:53  time: 0.8937  data_time: 0.0068  memory: 8456  loss: 2.5693\n04/13 10:57:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 230/2022]  lr: 1.9636e-05  eta: 0:30:33  time: 0.9002  data_time: 0.0064  memory: 8456  loss: 2.6244\n04/13 10:57:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 240/2022]  lr: 1.9592e-05  eta: 0:30:13  time: 0.8962  data_time: 0.0063  memory: 8456  loss: 2.6336\n04/13 10:57:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 250/2022]  lr: 1.9546e-05  eta: 0:29:54  time: 0.8908  data_time: 0.0064  memory: 8456  loss: 2.6051\n04/13 10:57:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 260/2022]  lr: 1.9497e-05  eta: 0:29:38  time: 0.9268  data_time: 0.0064  memory: 10226  loss: 2.5494\n04/13 10:57:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 270/2022]  lr: 1.9445e-05  eta: 0:29:20  time: 0.8915  data_time: 0.0072  memory: 10226  loss: 2.3168\n04/13 10:57:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 280/2022]  lr: 1.9391e-05  eta: 0:29:03  time: 0.8888  data_time: 0.0066  memory: 10226  loss: 2.5222\n04/13 10:58:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 290/2022]  lr: 1.9335e-05  eta: 0:28:46  time: 0.8891  data_time: 0.0068  memory: 10226  loss: 2.4886\n04/13 10:58:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 300/2022]  lr: 1.9277e-05  eta: 0:28:30  time: 0.8912  data_time: 0.0068  memory: 10226  loss: 2.4155\n04/13 10:58:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 10:58:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n压力是生活中不可避免的一部分，但是我们可以采取一些方法来缓解压力。首先，我们可以尝试深呼吸，放松身心，缓解紧张情绪。其次，我们可以尝试运动，释放身体的紧张情绪。此外，我们还可以尝试冥想，帮助我们集中注意力，减轻焦虑情绪。<|im_end|>\n\n04/13 10:58:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n很高兴能帮助您。首先，我需要了解一些关于您的具体信息，以便我能更好地理解您的问题。<|im_end|>\n\n04/13 10:58:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n首先，我要强调的是，每个人都有自己的优点和长处，羡慕别人并不意味着你不够优秀。每个人都有自己的独特之处，我们应该尊重并欣赏他人的优点。同时，我们也应该学会欣赏自己的优点，而不是过分地羡慕别人。<|im_end|>\n\n04/13 10:58:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 300 iterations\n[2024-04-13 10:58:16,526] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_300.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 10:58:16,536] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_300.pth/mp_rank_00_model_states.pt\n[2024-04-13 10:58:16,537] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_300.pth/mp_rank_00_model_states.pt...\n[2024-04-13 10:58:18,392] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_300.pth/mp_rank_00_model_states.pt.\n[2024-04-13 10:58:18,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_300.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 10:58:18,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_300.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 10:58:29,840] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_300.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 10:58:29,840] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_300.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 10:58:29,840] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_300.pth is ready now!\n[2024-04-13 10:58:30,737] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_300.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 10:58:37,055] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_300.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 10:58:37,055] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_300.pth is ready now!\n04/13 10:58:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 310/2022]  lr: 1.9216e-05  eta: 0:30:35  time: 3.4301  data_time: 2.5432  memory: 10226  loss: 2.4112\n04/13 10:58:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 320/2022]  lr: 1.9152e-05  eta: 0:30:16  time: 0.9249  data_time: 0.0079  memory: 10226  loss: 2.4049\n04/13 10:59:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 330/2022]  lr: 1.9087e-05  eta: 0:29:57  time: 0.9017  data_time: 0.0066  memory: 10226  loss: 2.1738\n04/13 10:59:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 340/2022]  lr: 1.9019e-05  eta: 0:29:38  time: 0.8993  data_time: 0.0068  memory: 10226  loss: 2.5629\n04/13 10:59:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 350/2022]  lr: 1.8948e-05  eta: 0:29:20  time: 0.8918  data_time: 0.0069  memory: 10226  loss: 2.2843\n04/13 10:59:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 360/2022]  lr: 1.8876e-05  eta: 0:29:02  time: 0.8884  data_time: 0.0068  memory: 10226  loss: 2.1800\n04/13 10:59:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 370/2022]  lr: 1.8801e-05  eta: 0:28:44  time: 0.8862  data_time: 0.0069  memory: 10226  loss: 2.2447\n04/13 10:59:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 380/2022]  lr: 1.8724e-05  eta: 0:28:27  time: 0.8848  data_time: 0.0065  memory: 10226  loss: 2.1406\n04/13 10:59:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 390/2022]  lr: 1.8644e-05  eta: 0:28:11  time: 0.9145  data_time: 0.0070  memory: 10226  loss: 2.1546\n04/13 11:00:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 400/2022]  lr: 1.8563e-05  eta: 0:27:55  time: 0.8851  data_time: 0.0064  memory: 10226  loss: 2.1517\n04/13 11:00:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 11:00:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n我能理解你的感受。压力是生活中不可避免的一部分，但是我们可以一起探讨一些应对压力的方法。首先，我们可以尝试一些放松技巧，比如深呼吸、瑜伽或者冥想，这些方法可以帮助你缓解紧张情绪。同时，我们也可以尝试一些正念冥想，这可以帮助你更好地理解自己的情绪，并学会如何管理它们。此外，保持良好的生活习惯，比如规律作息、合理饮食和适量运动，也能帮助你更好地应对压力。如果你觉得压力过大，我们可以一起制定一个更具体的计划，让你在压力面前更有信心。记住，每个人应对压力的方式都是不同的，重要的是找到最适合自己的方式。如果你在实施这些方法的过程中遇到任何困难，都可以随时来找我。<|im_end|>\n\n04/13 11:00:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n首先，我想告诉你，生活并不总是那么顺利。有时候，我们可能会感到生活乏味，甚至感到沮丧。但是，我们不能因此就放弃。首先，我想告诉你，每个人都有自己的节奏和方式来应对生活。你可以尝试一些放松技巧，比如深呼吸、冥想或者瑜伽，来帮助你缓解压力。同时，保持积极的心态也很重要。你可以尝试将注意力转移到积极的事情上，比如和朋友分享你的快乐，或者做一些你喜欢的事情。记住，改变需要时间和耐心，不要对自己太苛刻。如果你觉得生活没有意义，我们可以一起探讨一些可能的解决方案，比如设定一些小目标，或者尝试一些新的爱好。记住，每个人的生活都是独特的，不必强求自己必须按照别人的方式生活。<|im_end|>\n\n04/13 11:00:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n首先，我想告诉你，每个人都有自己的优点和独特之处，羡慕别人并不意味着你不够好。每个人都有自己的价值和潜力，我们应该尊重和欣赏他人的优点，而不是过分关注别人的缺点。同时，我们也应该学会欣赏自己的优点，而不是过分关注别人的优点。如果你在羡慕别人时感到困扰，可以尝试从自己的角度去思考，看看自己是否真的需要羡慕别人。<|im_end|>\n\n04/13 11:00:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 400 iterations\n[2024-04-13 11:00:18,828] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_400.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 11:00:18,839] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_400.pth/mp_rank_00_model_states.pt\n[2024-04-13 11:00:18,839] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_400.pth/mp_rank_00_model_states.pt...\n[2024-04-13 11:00:20,728] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_400.pth/mp_rank_00_model_states.pt.\n[2024-04-13 11:00:20,730] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_400.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 11:00:20,730] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_400.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 11:00:29,351] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_400.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 11:00:29,351] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_400.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 11:00:29,351] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_400.pth is ready now!\n[2024-04-13 11:00:31,182] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_400.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 11:00:38,283] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_400.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 11:00:38,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_400.pth is ready now!\n04/13 11:00:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 410/2022]  lr: 1.8479e-05  eta: 0:29:43  time: 4.0463  data_time: 3.1603  memory: 10226  loss: 2.1565\n04/13 11:00:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 420/2022]  lr: 1.8393e-05  eta: 0:29:24  time: 0.8961  data_time: 0.0082  memory: 10226  loss: 2.1908\n04/13 11:01:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 430/2022]  lr: 1.8305e-05  eta: 0:29:05  time: 0.9007  data_time: 0.0070  memory: 10226  loss: 2.1930\n04/13 11:01:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 440/2022]  lr: 1.8214e-05  eta: 0:28:47  time: 0.8968  data_time: 0.0073  memory: 10226  loss: 2.2036\n04/13 11:01:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 450/2022]  lr: 1.8122e-05  eta: 0:28:30  time: 0.9183  data_time: 0.0068  memory: 10226  loss: 2.0808\n04/13 11:01:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 460/2022]  lr: 1.8028e-05  eta: 0:28:12  time: 0.8867  data_time: 0.0074  memory: 10226  loss: 2.1353\n04/13 11:01:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 470/2022]  lr: 1.7931e-05  eta: 0:27:55  time: 0.8821  data_time: 0.0069  memory: 10226  loss: 2.1206\n04/13 11:01:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 480/2022]  lr: 1.7833e-05  eta: 0:27:38  time: 0.8811  data_time: 0.0070  memory: 10226  loss: 2.1625\n04/13 11:01:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 490/2022]  lr: 1.7732e-05  eta: 0:27:21  time: 0.8842  data_time: 0.0067  memory: 10226  loss: 2.0418\n04/13 11:02:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 500/2022]  lr: 1.7629e-05  eta: 0:27:05  time: 0.8861  data_time: 0.0072  memory: 10226  loss: 2.0512\n04/13 11:02:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 11:02:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，压力是生活中常见的现象，但是过度的压力会对我们的身心健康产生负面影响。首先，我想告诉你，压力是正常的生理反应，它可以帮助我们更好地应对挑战。你可以尝试一些放松技巧，比如深呼吸、冥想或者进行一些你喜欢的活动，来缓解压力。同时，保持良好的生活习惯，如规律作息、合理饮食和适量运动，也能帮助你更好地应对压力。如果你觉得压力过大，可以考虑寻求专业的心理咨询师的帮助，他们可以提供更个性化的建议和策略。记住，压力是成长的一部分，关键是如何管理和应对它。祝你早日走出困境，过上更健康、更快乐的生活。<|im_end|>\n\n04/13 11:02:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n首先，我想告诉你，生活并不总是那么顺利，有时候会遇到一些挫折和困难。你可以尝试从以下几个方面来调整自己的心态：\n\n1. 保持积极乐观的心态：面对生活中的困难和挫折，我们可以尝试从积极的角度去看待问题，而不是一味地抱怨和抱怨。\n\n2. 建立良好的人际关系：与家人、朋友保持良好的沟通，分享自己的困扰和感受，他们可以给你提供支持和帮助。\n\n3. 学会放松：适当的运动、冥想、阅读等都可以帮助你缓解压力，提高生活质量。\n\n4. 保持健康的生活习惯：合理的饮食、规律的作息和适量的运动，都可以帮助你保持良好的身心状态。\n\n如果你觉得生活没有意思，可以尝试以上的方法，如果问题持续存在，建议你寻求专业的心理咨询师的帮助。记住，心理健康是个人成长的重要组成部分，如果你感到困扰，不要犹豫寻求帮助。<|im_end|>\n\n04/13 11:02:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n羡慕别人是一种常见的心理现象，但过度的羡慕可能会导致焦虑、抑郁等心理问题。首先，我们应该认识到，每个人都有自己的优点和独特之处，我们应该欣赏和尊重这些优点。其次，我们可以尝试从自己的角度去看待问题，而不是过分关注他人的优点。最后，我们可以尝试进行一些放松训练，如深呼吸、冥想等，帮助自己缓解压力。<|im_end|>\n\n04/13 11:02:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 500 iterations\n[2024-04-13 11:02:20,400] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_500.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 11:02:20,410] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_500.pth/mp_rank_00_model_states.pt\n[2024-04-13 11:02:20,410] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_500.pth/mp_rank_00_model_states.pt...\n[2024-04-13 11:02:22,271] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_500.pth/mp_rank_00_model_states.pt.\n[2024-04-13 11:02:22,273] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_500.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 11:02:22,273] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_500.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 11:02:31,108] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_500.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 11:02:31,109] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_500.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 11:02:31,109] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!\n[2024-04-13 11:02:32,579] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_500.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 11:02:40,498] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_500.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 11:02:40,498] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!\n04/13 11:02:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 510/2022]  lr: 1.7525e-05  eta: 0:28:26  time: 4.1848  data_time: 3.3007  memory: 10226  loss: 2.1235\n04/13 11:02:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 520/2022]  lr: 1.7419e-05  eta: 0:28:09  time: 0.9209  data_time: 0.0069  memory: 10226  loss: 2.0353\n04/13 11:03:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 530/2022]  lr: 1.7310e-05  eta: 0:27:51  time: 0.8970  data_time: 0.0072  memory: 10226  loss: 2.0322\n04/13 11:03:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 540/2022]  lr: 1.7200e-05  eta: 0:27:34  time: 0.8920  data_time: 0.0067  memory: 10226  loss: 2.0056\n04/13 11:03:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 550/2022]  lr: 1.7088e-05  eta: 0:27:17  time: 0.8887  data_time: 0.0068  memory: 10226  loss: 2.1939\n04/13 11:03:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 560/2022]  lr: 1.6974e-05  eta: 0:27:00  time: 0.8840  data_time: 0.0070  memory: 10226  loss: 1.9856\n04/13 11:03:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 570/2022]  lr: 1.6859e-05  eta: 0:26:43  time: 0.8800  data_time: 0.0065  memory: 10226  loss: 2.0820\n04/13 11:03:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 580/2022]  lr: 1.6741e-05  eta: 0:26:27  time: 0.9110  data_time: 0.0067  memory: 10226  loss: 2.0138\n04/13 11:04:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 590/2022]  lr: 1.6622e-05  eta: 0:26:11  time: 0.8831  data_time: 0.0068  memory: 10226  loss: 2.1750\n04/13 11:04:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 600/2022]  lr: 1.6501e-05  eta: 0:25:55  time: 0.8867  data_time: 0.0073  memory: 10226  loss: 1.9852\n04/13 11:04:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 11:04:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，压力确实会让人感到焦虑和不安。首先，我想告诉你，压力是正常的，每个人都有过类似的经历。你可以尝试一些放松技巧，比如深呼吸、冥想或者进行一些你喜欢的活动，来帮助你缓解压力。此外，如果压力持续存在，建议你寻求专业的心理咨询师的帮助，他们可以提供更个性化的建议和治疗方案。记住，心理健康是个人成长的重要部分，不要因为一时的困难就否定自己。<|im_end|>\n\n04/13 11:04:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，生活确实会让人感到无趣。首先，我想告诉你，生活中的无趣并不意味着你不够好，而是你正在经历一些挑战。你可以尝试从以下几个方面来调整自己的心态：一是设定合理的目标，不要给自己太大压力；二是学会放松，比如每天花点时间做自己喜欢的事情；三是保持积极的心态，相信自己有能力克服困难。如果你觉得这些方法对你有所帮助，那么请继续坚持下去，相信你会找到属于自己的乐趣。<|im_end|>\n\n04/13 11:04:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n羡慕别人是一种正常的情感反应，但过度的羡慕可能会导致心理压力和焦虑。你可以尝试以下方法来缓解这种情绪：\n\n1. 自我肯定：每天对自己说一些积极的话，告诉自己你有能力去实现自己的梦想。\n\n2. 与他人分享：与朋友、家人分享你的感受，让他们了解你的困扰，也可以从他们的经验中学习。\n\n3. 保持健康的生活方式：保证充足的睡眠，均衡的饮食，适度的运动，这些都有助于缓解压力。\n\n4. 学会放松：学习一些放松技巧，如深呼吸、冥想等，帮助你缓解焦虑。\n\n5. 寻求专业帮助：如果你发现自己无法独自应对这种情绪，可以考虑寻求心理咨询师的帮助。\n\n记住，改变需要时间和耐心，不要对自己过于苛刻。如果你在尝试这些方法后仍然感到困扰，或者有其他心理问题，欢迎随时来找我。<|im_end|>\n\n04/13 11:04:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 600 iterations\n[2024-04-13 11:04:21,975] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_600.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 11:04:21,985] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_600.pth/mp_rank_00_model_states.pt\n[2024-04-13 11:04:21,986] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_600.pth/mp_rank_00_model_states.pt...\n[2024-04-13 11:04:23,885] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_600.pth/mp_rank_00_model_states.pt.\n[2024-04-13 11:04:23,887] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_600.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 11:04:23,887] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_600.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 11:04:32,046] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_600.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 11:04:32,046] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_600.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 11:04:32,046] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_600.pth is ready now!\n[2024-04-13 11:04:32,572] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_600.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 11:04:48,409] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_600.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 11:04:48,409] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_600.pth is ready now!\n04/13 11:04:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 610/2022]  lr: 1.6379e-05  eta: 0:27:08  time: 4.7499  data_time: 3.8628  memory: 10226  loss: 2.2921\n04/13 11:05:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 620/2022]  lr: 1.6255e-05  eta: 0:26:51  time: 0.8942  data_time: 0.0069  memory: 10226  loss: 1.9643\n04/13 11:05:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 630/2022]  lr: 1.6129e-05  eta: 0:26:34  time: 0.8962  data_time: 0.0079  memory: 10226  loss: 1.9240\n04/13 11:05:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 640/2022]  lr: 1.6002e-05  eta: 0:26:18  time: 0.9197  data_time: 0.0067  memory: 10226  loss: 2.0365\n04/13 11:05:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 650/2022]  lr: 1.5873e-05  eta: 0:26:01  time: 0.8858  data_time: 0.0065  memory: 10226  loss: 1.8994\n04/13 11:05:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 660/2022]  lr: 1.5742e-05  eta: 0:25:44  time: 0.8841  data_time: 0.0074  memory: 10226  loss: 1.9939\n04/13 11:05:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 670/2022]  lr: 1.5610e-05  eta: 0:25:28  time: 0.8820  data_time: 0.0068  memory: 10226  loss: 1.8595\n04/13 11:05:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: qwen1_5_0_5_B_full_20240413_105218\n04/13 11:05:54 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Reach the end of the dataloader, it will be restarted and continue to iterate. It is recommended to use `mmengine.dataset.InfiniteSampler` to enable the dataloader to iterate infinitely.\n04/13 11:06:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 680/2022]  lr: 1.5477e-05  eta: 0:25:16  time: 1.0895  data_time: 0.2071  memory: 10226  loss: 2.0407\n04/13 11:06:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 690/2022]  lr: 1.5343e-05  eta: 0:25:00  time: 0.8848  data_time: 0.0069  memory: 10226  loss: 1.8565\n04/13 11:06:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 700/2022]  lr: 1.5207e-05  eta: 0:24:44  time: 0.8882  data_time: 0.0067  memory: 10226  loss: 2.0008\n04/13 11:06:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 11:06:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，压力确实会让人感到焦虑和不安。首先，我想告诉你，压力是正常的，每个人都有过类似的经历。你能告诉我，具体是哪些事情让你感到压力吗？<|im_end|>\n\n04/13 11:06:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n我理解你的感受，生活确实会让人感到无趣。首先，我想告诉你，每个人都有自己的节奏和喜好，不必强求自己与他人完全相同。你可以尝试从以下几个方面来调整自己的心态：1. 重新审视自己的生活目标，看看是否与当前的生活状态有所冲突；2. 与亲朋好友分享你的感受，听听他们的建议；3. 学会放松，比如进行一些自己喜欢的活动，或者进行一些深呼吸练习。记住，改变需要时间，不要给自己太大压力。如果你觉得有必要，我们可以一起探讨一些更具体的心理调适方法。<|im_end|>\n\n04/13 11:06:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n羡慕别人是一种正常的情感反应，但过度的羡慕可能会导致心理压力和焦虑。建议你尝试以下方法来缓解这种情绪：\n\n1. 自我肯定：每天对自己说一些积极的话，比如“我有能力去实现我的梦想”，“我值得拥有更好的生活”。\n\n2. 与他人交流：与朋友、家人分享你的感受，让他们了解你的困扰，也可以从他们的经验中学习。\n\n3. 适当休息：保证足够的睡眠，避免过度劳累。\n\n4. 学会感恩：学会欣赏自己的优点，而不是过分关注别人的成就。\n\n5. 寻求专业帮助：如果情绪持续影响到日常生活，可以考虑寻求心理咨询师的帮助。\n\n记住，改变需要时间，不要对自己太过苛刻。同时，保持健康的生活方式和积极的心态也是非常重要的。<|im_end|>\n\n04/13 11:06:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 700 iterations\n[2024-04-13 11:06:30,035] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_700.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 11:06:30,045] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_700.pth/mp_rank_00_model_states.pt\n[2024-04-13 11:06:30,045] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_700.pth/mp_rank_00_model_states.pt...\n[2024-04-13 11:06:31,930] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_700.pth/mp_rank_00_model_states.pt.\n[2024-04-13 11:06:31,931] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_700.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 11:06:31,931] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_700.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 11:06:43,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_700.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 11:06:43,919] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_700.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 11:06:43,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_700.pth is ready now!\n[2024-04-13 11:06:46,475] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_700.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 11:06:46,489] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_700.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 11:06:46,489] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_700.pth is ready now!\n04/13 11:06:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 710/2022]  lr: 1.5069e-05  eta: 0:25:19  time: 3.6116  data_time: 2.6969  memory: 10226  loss: 1.9275\n04/13 11:07:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 720/2022]  lr: 1.4931e-05  eta: 0:25:02  time: 0.8920  data_time: 0.0073  memory: 10226  loss: 1.9000\n04/13 11:07:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 730/2022]  lr: 1.4791e-05  eta: 0:24:46  time: 0.8980  data_time: 0.0070  memory: 10226  loss: 1.8993\n04/13 11:07:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 740/2022]  lr: 1.4649e-05  eta: 0:24:30  time: 0.8977  data_time: 0.0072  memory: 10226  loss: 1.9055\n04/13 11:07:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 750/2022]  lr: 1.4507e-05  eta: 0:24:15  time: 0.9135  data_time: 0.0065  memory: 10234  loss: 2.1037\n04/13 11:07:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 760/2022]  lr: 1.4364e-05  eta: 0:23:59  time: 0.8875  data_time: 0.0065  memory: 10226  loss: 2.0796\n04/13 11:07:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 770/2022]  lr: 1.4219e-05  eta: 0:23:44  time: 0.9122  data_time: 0.0067  memory: 10226  loss: 1.9990\n04/13 11:07:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 780/2022]  lr: 1.4073e-05  eta: 0:23:28  time: 0.8833  data_time: 0.0068  memory: 10226  loss: 1.8134\n04/13 11:08:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 790/2022]  lr: 1.3926e-05  eta: 0:23:13  time: 0.8827  data_time: 0.0065  memory: 10226  loss: 2.0804\n04/13 11:08:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 800/2022]  lr: 1.3779e-05  eta: 0:22:58  time: 0.8848  data_time: 0.0067  memory: 10226  loss: 1.8814\n04/13 11:08:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 11:08:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，压力确实会让人感到焦虑和疲惫。首先，我想告诉你，每个人在面对压力时都会感到焦虑，这是正常的。你能告诉我，当你感到压力时，通常会想到哪些事情吗？<|im_end|>\n\n04/13 11:08:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n我理解你的感受，生活确实可能让人感到无趣。首先，我想告诉你，每个人都有自己的节奏和喜好，不必强求与他人比较。你可以尝试以下方法来调整自己的心态：\n\n1. 了解自己的兴趣爱好，找到自己真正热爱的事情。\n2. 与朋友、家人分享你的感受，他们或许能给你带来新的视角和建议。\n3. 学会放松，比如进行一些你喜欢的活动，如阅读、运动等。\n4. 与专业人士沟通，寻求专业的心理咨询帮助。\n\n请记住，改变需要时间，不要给自己太大压力。同时，也要学会接受自己的不完美，因为这是成长的一部分。<|im_end|>\n\n04/13 11:08:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n我理解你的感受，羡慕别人是一种很自然的情感，但过度的羡慕可能会导致一些问题。首先，我想告诉你，每个人都有自己的优点和不足，羡慕别人并不代表你自己的价值。你可以试着从自己的优点和成就中找到满足感，而不是过分关注别人的成就。此外，我们可以一起探讨一些方法来帮助你更好地管理这种情绪，比如进行正念冥想、正念呼吸练习等。<|im_end|>\n\n04/13 11:08:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 800 iterations\n[2024-04-13 11:08:24,876] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_800.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 11:08:24,896] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_800.pth/mp_rank_00_model_states.pt\n[2024-04-13 11:08:24,896] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_800.pth/mp_rank_00_model_states.pt...\n[2024-04-13 11:08:26,855] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_800.pth/mp_rank_00_model_states.pt.\n[2024-04-13 11:08:26,857] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_800.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 11:08:26,857] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_800.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 11:08:34,425] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_800.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 11:08:34,425] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_800.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 11:08:34,425] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_800.pth is ready now!\n[2024-04-13 11:08:35,097] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_800.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 11:08:35,128] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_800.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 11:08:35,129] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_800.pth is ready now!\n04/13 11:08:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 810/2022]  lr: 1.3630e-05  eta: 0:23:12  time: 2.7818  data_time: 1.8968  memory: 10226  loss: 1.9873\n04/13 11:08:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 820/2022]  lr: 1.3480e-05  eta: 0:22:56  time: 0.8951  data_time: 0.0077  memory: 10226  loss: 1.9327\n04/13 11:09:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 830/2022]  lr: 1.3330e-05  eta: 0:22:41  time: 0.9018  data_time: 0.0066  memory: 10226  loss: 2.0302\n04/13 11:09:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 840/2022]  lr: 1.3178e-05  eta: 0:22:27  time: 0.9276  data_time: 0.0067  memory: 10226  loss: 1.9549\n04/13 11:09:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 850/2022]  lr: 1.3026e-05  eta: 0:22:12  time: 0.8920  data_time: 0.0069  memory: 10226  loss: 1.7967\n04/13 11:09:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 860/2022]  lr: 1.2873e-05  eta: 0:21:57  time: 0.8887  data_time: 0.0066  memory: 10226  loss: 1.9275\n04/13 11:09:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 870/2022]  lr: 1.2719e-05  eta: 0:21:43  time: 0.8851  data_time: 0.0066  memory: 10226  loss: 1.8094\n04/13 11:09:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 880/2022]  lr: 1.2565e-05  eta: 0:21:28  time: 0.8832  data_time: 0.0065  memory: 10226  loss: 1.7687\n04/13 11:09:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 890/2022]  lr: 1.2410e-05  eta: 0:21:14  time: 0.8852  data_time: 0.0067  memory: 10226  loss: 1.8505\n04/13 11:10:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 900/2022]  lr: 1.2254e-05  eta: 0:21:00  time: 0.9171  data_time: 0.0069  memory: 10226  loss: 1.8071\n04/13 11:10:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 11:10:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，面对压力确实会让人感到焦虑和疲惫。首先，我想告诉你，每个人在面对压力时都会有不同的应对方式。你能告诉我，当你感到压力时，通常会想到哪些事情？<|im_end|>\n\n04/13 11:10:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，生活确实会带来一些压力和困扰。首先，我想告诉你，每个人在面对生活中的挑战时，都会有不同的应对方式。你能告诉我，当你感到生活没意思时，通常会有什么样的想法或者感受吗？<|im_end|>\n\n04/13 11:10:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n我理解你的感受，羡慕别人是一种正常的情感反应，但过度的羡慕可能会导致压力和焦虑。你可以尝试以下方法来缓解这种情绪：\n\n1. 自我肯定：每天对自己说一些积极的话，提醒自己自己的价值和能力。\n\n2. 了解自己的优点：认识到自己的优点和长处，而不是过分关注别人的优点。\n\n3. 与他人分享：与朋友、家人分享你的感受，他们可能会给你提供不同的视角和建议。\n\n4. 学会接受：接受自己的不完美，每个人都有自己的优点和不足。\n\n5. 保持健康的生活方式：保证充足的睡眠、均衡的饮食和适量的运动，这些都有助于改善情绪。\n\n如果你发现自己很难控制这种情绪，或者感到焦虑和压力，可以考虑寻求专业的心理咨询帮助。记住，心理健康和生活质量同样重要，不要因为别人的羡慕而忽视了自己。<|im_end|>\n\n04/13 11:10:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 900 iterations\n[2024-04-13 11:10:13,543] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_900.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 11:10:13,553] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_900.pth/mp_rank_00_model_states.pt\n[2024-04-13 11:10:13,553] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_900.pth/mp_rank_00_model_states.pt...\n[2024-04-13 11:10:15,612] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_900.pth/mp_rank_00_model_states.pt.\n[2024-04-13 11:10:15,615] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_900.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 11:10:15,615] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_900.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 11:10:23,522] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_900.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 11:10:23,522] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_900.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 11:10:23,522] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_900.pth is ready now!\n[2024-04-13 11:10:23,760] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_900.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 11:10:23,760] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_900.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 11:10:23,760] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_900.pth is ready now!\n04/13 11:10:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 910/2022]  lr: 1.2098e-05  eta: 0:21:09  time: 2.7832  data_time: 1.9046  memory: 10226  loss: 1.7528\n04/13 11:10:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 920/2022]  lr: 1.1941e-05  eta: 0:20:55  time: 0.8932  data_time: 0.0085  memory: 10226  loss: 1.7921\n04/13 11:10:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 930/2022]  lr: 1.1784e-05  eta: 0:20:41  time: 0.9014  data_time: 0.0067  memory: 10226  loss: 1.7749\n04/13 11:10:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 940/2022]  lr: 1.1626e-05  eta: 0:20:26  time: 0.9025  data_time: 0.0077  memory: 10226  loss: 1.8107\n04/13 11:11:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 950/2022]  lr: 1.1468e-05  eta: 0:20:12  time: 0.8920  data_time: 0.0069  memory: 10226  loss: 1.8669\n04/13 11:11:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 960/2022]  lr: 1.1309e-05  eta: 0:19:59  time: 0.9203  data_time: 0.0065  memory: 10226  loss: 1.8282\n04/13 11:11:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 970/2022]  lr: 1.1150e-05  eta: 0:19:45  time: 0.8860  data_time: 0.0071  memory: 10226  loss: 1.7984\n04/13 11:11:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 980/2022]  lr: 1.0991e-05  eta: 0:19:31  time: 0.8841  data_time: 0.0069  memory: 10226  loss: 1.7436\n04/13 11:11:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [ 990/2022]  lr: 1.0832e-05  eta: 0:19:17  time: 0.8832  data_time: 0.0066  memory: 10226  loss: 1.7412\n04/13 11:11:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: qwen1_5_0_5_B_full_20240413_105218\n04/13 11:11:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1000/2022]  lr: 1.0672e-05  eta: 0:19:04  time: 0.8843  data_time: 0.0066  memory: 10226  loss: 1.7035\n04/13 11:11:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 11:11:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，面对压力确实会让人感到焦虑和疲惫。首先，你能告诉我，是什么具体的事情让你感到压力吗？<|im_end|>\n\n04/13 11:11:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，生活确实可能让人感到无趣。首先，我想告诉你，每个人都有自己的节奏和兴趣，不必强求与他人比较。你可以尝试调整一下自己的心态，给自己一些积极的暗示，比如“我可以做到的”，“我有能力去实现我的目标”。此外，也可以尝试一些放松的方法，比如深呼吸、冥想或者散步，这些都能帮助你缓解压力。如果情况持续恶化，也可以考虑寻求专业的心理咨询帮助。记住，心理健康和生活质量同样重要，不要让生活压力过大影响到你的身心健康。<|im_end|>\n\n04/13 11:12:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n我理解你的感受，羡慕别人是一种正常的情感反应，但过度的羡慕可能会导致压力和焦虑。你可以尝试以下方法来缓解这种情绪：\n\n1. 自我肯定：每天对自己说一些积极的话，提醒自己自己的价值和能力。\n\n2. 增强自信心：通过学习新技能、提升自我价值等方式来增强自信心。\n\n3. 与他人交流：与朋友、家人分享你的感受，他们可能会给你提供不同的视角和建议。\n\n4. 保持健康的生活方式：保证充足的睡眠、均衡的饮食和适量的运动，这些都有助于改善情绪。\n\n如果你发现自己很难控制这种情绪，或者情绪持续影响到日常生活，建议你寻求专业的心理咨询帮助。记住，心理健康和生活质量同样重要，不要因为别人的羡慕而忽视了自我照顾。<|im_end|>\n\n04/13 11:12:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1000 iterations\n[2024-04-13 11:12:03,431] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_1000.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 11:12:03,442] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1000.pth/mp_rank_00_model_states.pt\n[2024-04-13 11:12:03,442] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1000.pth/mp_rank_00_model_states.pt...\n[2024-04-13 11:12:05,368] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1000.pth/mp_rank_00_model_states.pt.\n[2024-04-13 11:12:05,370] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1000.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 11:12:05,370] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1000.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 11:12:13,220] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1000.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 11:12:15,223] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1000.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 11:12:15,223] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1000.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 11:12:15,223] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!\n[2024-04-13 11:12:22,403] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1000.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 11:12:22,403] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!\n04/13 11:12:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1010/2022]  lr: 1.0512e-05  eta: 0:19:19  time: 3.8219  data_time: 2.9362  memory: 10226  loss: 1.7036\n04/13 11:12:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1020/2022]  lr: 1.0352e-05  eta: 0:19:06  time: 0.8951  data_time: 0.0072  memory: 10226  loss: 1.8072\n04/13 11:12:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1030/2022]  lr: 1.0192e-05  eta: 0:18:52  time: 0.9282  data_time: 0.0066  memory: 10226  loss: 1.7710\n04/13 11:12:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1040/2022]  lr: 1.0032e-05  eta: 0:18:38  time: 0.8956  data_time: 0.0068  memory: 10226  loss: 1.8231\n04/13 11:13:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1050/2022]  lr: 9.8719e-06  eta: 0:18:25  time: 0.8914  data_time: 0.0068  memory: 10226  loss: 1.8264\n04/13 11:13:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1060/2022]  lr: 9.7118e-06  eta: 0:18:11  time: 0.8870  data_time: 0.0069  memory: 10226  loss: 1.8842\n04/13 11:13:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1070/2022]  lr: 9.5518e-06  eta: 0:17:57  time: 0.8857  data_time: 0.0066  memory: 10226  loss: 1.7320\n04/13 11:13:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1080/2022]  lr: 9.3919e-06  eta: 0:17:44  time: 0.8849  data_time: 0.0071  memory: 10226  loss: 1.7461\n04/13 11:13:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1090/2022]  lr: 9.2322e-06  eta: 0:17:31  time: 0.9144  data_time: 0.0068  memory: 10226  loss: 1.8346\n04/13 11:13:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1100/2022]  lr: 9.0726e-06  eta: 0:17:17  time: 0.8866  data_time: 0.0065  memory: 10226  loss: 1.7264\n04/13 11:13:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 11:13:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，面对压力确实会让人感到焦虑和疲惫。首先，我想告诉你，你并不孤单，很多人都会有类似的经历。你能告诉我，是什么让你感到压力那么大吗？<|im_end|>\n\n04/13 11:13:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，生活确实可能让人感到无趣。首先，我想告诉你，每个人在面对生活中的挑战时，都会有不同的反应。你能告诉我，是什么让你感到无趣吗？<|im_end|>\n\n04/13 11:13:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n我理解你的感受，羡慕别人是一种正常的情感反应，但过度的羡慕可能会导致心理压力和焦虑。首先，我想告诉你，每个人都有自己的优点和不足，羡慕别人并不代表你自己的价值。你可以尝试从以下几个方面来调整自己的心态：1. 自我接纳，接纳自己的优点和不足；2. 与他人分享自己的快乐和成就，让别人看到你的积极面；3. 学会欣赏自己，欣赏自己的努力和进步。<|im_end|>\n\n04/13 11:13:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1100 iterations\n[2024-04-13 11:13:58,014] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_1100.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 11:13:58,025] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1100.pth/mp_rank_00_model_states.pt\n[2024-04-13 11:13:58,025] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1100.pth/mp_rank_00_model_states.pt...\n[2024-04-13 11:13:59,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1100.pth/mp_rank_00_model_states.pt.\n[2024-04-13 11:13:59,951] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1100.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 11:13:59,951] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1100.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 11:14:07,922] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1100.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 11:14:07,922] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1100.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 11:14:07,922] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1100.pth is ready now!\n[2024-04-13 11:14:09,994] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1100.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 11:14:15,870] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1100.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 11:14:15,870] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1100.pth is ready now!\n04/13 11:14:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1110/2022]  lr: 8.9133e-06  eta: 0:17:24  time: 3.2768  data_time: 2.3924  memory: 10226  loss: 1.7693\n04/13 11:14:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1120/2022]  lr: 8.7543e-06  eta: 0:17:10  time: 0.8932  data_time: 0.0075  memory: 10226  loss: 1.8218\n04/13 11:14:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1130/2022]  lr: 8.5956e-06  eta: 0:16:57  time: 0.8998  data_time: 0.0071  memory: 10226  loss: 1.8352\n04/13 11:14:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1140/2022]  lr: 8.4372e-06  eta: 0:16:44  time: 0.8944  data_time: 0.0065  memory: 10226  loss: 1.8384\n04/13 11:15:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1150/2022]  lr: 8.2793e-06  eta: 0:16:30  time: 0.8902  data_time: 0.0067  memory: 10226  loss: 1.9158\n04/13 11:15:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1160/2022]  lr: 8.1218e-06  eta: 0:16:17  time: 0.9155  data_time: 0.0071  memory: 10226  loss: 1.7897\n04/13 11:15:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1170/2022]  lr: 7.9648e-06  eta: 0:16:04  time: 0.8818  data_time: 0.0066  memory: 10226  loss: 1.6746\n04/13 11:15:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1180/2022]  lr: 7.8083e-06  eta: 0:15:51  time: 0.8810  data_time: 0.0065  memory: 10226  loss: 1.7044\n04/13 11:15:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1190/2022]  lr: 7.6523e-06  eta: 0:15:38  time: 0.8832  data_time: 0.0065  memory: 10226  loss: 1.9027\n04/13 11:15:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1200/2022]  lr: 7.4970e-06  eta: 0:15:25  time: 0.8855  data_time: 0.0070  memory: 10226  loss: 1.7163\n04/13 11:15:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 11:15:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，面对压力确实会让人感到很不舒服。首先，我想告诉你，你并不孤单，很多人都会有类似的经历。你能告诉我，是什么让你感到压力那么大吗？<|im_end|>\n\n04/13 11:15:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，生活确实可能让人感到无趣。首先，我想告诉你，每个人在面对生活中的挑战时，都会有不同的反应。你能告诉我，是什么让你感到生活没有意思吗？<|im_end|>\n\n04/13 11:15:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n我理解你的感受，羡慕别人是一种很自然的情感反应。首先，我想告诉你，每个人都有自己的优点和不足，羡慕别人并不代表你不够好。你可以尝试从以下几个方面来调整自己的心态：1. 自我肯定，相信自己的价值；2. 与他人比较时，要客观看待，不要过分比较；3. 专注于自己的成长和进步，而不是过分关注他人的成就。<|im_end|>\n\n04/13 11:15:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1200 iterations\n[2024-04-13 11:15:50,753] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_1200.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 11:15:50,764] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1200.pth/mp_rank_00_model_states.pt\n[2024-04-13 11:15:50,764] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1200.pth/mp_rank_00_model_states.pt...\n[2024-04-13 11:15:52,754] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1200.pth/mp_rank_00_model_states.pt.\n[2024-04-13 11:15:52,755] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1200.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 11:15:52,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1200.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 11:16:00,372] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1200.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 11:16:02,850] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1200.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 11:16:02,850] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1200.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 11:16:02,850] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1200.pth is ready now!\n[2024-04-13 11:16:07,799] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1200.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 11:16:07,799] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1200.pth is ready now!\n04/13 11:16:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1210/2022]  lr: 7.3423e-06  eta: 0:15:28  time: 3.1687  data_time: 2.2833  memory: 10226  loss: 1.8256\n04/13 11:16:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1220/2022]  lr: 7.1883e-06  eta: 0:15:15  time: 0.9211  data_time: 0.0071  memory: 10226  loss: 1.7454\n04/13 11:16:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1230/2022]  lr: 7.0350e-06  eta: 0:15:02  time: 0.8974  data_time: 0.0066  memory: 10226  loss: 1.7369\n04/13 11:16:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1240/2022]  lr: 6.8824e-06  eta: 0:14:49  time: 0.8946  data_time: 0.0067  memory: 10226  loss: 1.6641\n04/13 11:16:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1250/2022]  lr: 6.7307e-06  eta: 0:14:36  time: 0.8902  data_time: 0.0069  memory: 10226  loss: 1.7017\n04/13 11:17:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1260/2022]  lr: 6.5798e-06  eta: 0:14:23  time: 0.8876  data_time: 0.0069  memory: 10226  loss: 2.0318\n04/13 11:17:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1270/2022]  lr: 6.4298e-06  eta: 0:14:10  time: 0.8833  data_time: 0.0067  memory: 10226  loss: 1.7841\n04/13 11:17:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1280/2022]  lr: 6.2807e-06  eta: 0:13:58  time: 0.9125  data_time: 0.0069  memory: 10226  loss: 1.8670\n04/13 11:17:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1290/2022]  lr: 6.1325e-06  eta: 0:13:45  time: 0.8824  data_time: 0.0068  memory: 10226  loss: 1.6836\n04/13 11:17:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1300/2022]  lr: 5.9854e-06  eta: 0:13:32  time: 0.8862  data_time: 0.0066  memory: 10226  loss: 1.7704\n04/13 11:17:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - after_train_iter in EvaluateChatHook.\n04/13 11:17:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我压力很大<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，面对压力确实会让人感到很不舒服。首先，我想告诉你，你并不孤单，很多人都会有类似的经历。你能告诉我，具体是哪些事情让你感到压力吗？<|im_end|>\n\n04/13 11:17:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n生活没意思<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，生活确实可能让人感到无趣。首先，我想告诉你，每个人在不同阶段都会有这样的感觉。你能告诉我，是什么让你感到生活没有意思吗？<|im_end|>\n\n04/13 11:17:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n非常容易羡慕别人啊<|im_end|>\n<|im_start|>assistant\n我理解你的感受，羡慕别人是一种很自然的情感反应。首先，我想告诉你，每个人都有自己的优点和不足，羡慕别人并不代表你不够好。你可以试着从以下几个方面来调整自己的心态：1. 自我肯定，相信自己的价值；2. 专注于自己的成长，而不是过分关注他人的成就；3. 学会欣赏自己的优点，而不是过分关注别人的缺点。<|im_end|>\n\n04/13 11:17:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1300 iterations\n[2024-04-13 11:17:42,869] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_1300.pth is about to be saved!\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n  warnings.warn(\n[2024-04-13 11:17:42,879] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1300.pth/mp_rank_00_model_states.pt\n[2024-04-13 11:17:42,879] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1300.pth/mp_rank_00_model_states.pt...\n[2024-04-13 11:17:45,242] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1300.pth/mp_rank_00_model_states.pt.\n[2024-04-13 11:17:45,244] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1300.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt...\n[2024-04-13 11:17:45,244] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1300.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt...\n[2024-04-13 11:17:53,468] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1300.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt.\n[2024-04-13 11:17:53,468] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1300.pth/zero_pp_rank_1_mp_rank_00_optim_states.pt\n[2024-04-13 11:17:53,468] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1300.pth is ready now!\n[2024-04-13 11:17:55,102] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1300.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt.\n[2024-04-13 11:18:04,373] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved /kaggle/working/work_dirs/qwen1_5_0_5_B_full/iter_1300.pth/zero_pp_rank_0_mp_rank_00_optim_states.pt\n[2024-04-13 11:18:04,373] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1300.pth is ready now!\n04/13 11:18:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1310/2022]  lr: 5.8392e-06  eta: 0:13:35  time: 3.6001  data_time: 2.7167  memory: 10226  loss: 1.6299\n04/13 11:18:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1320/2022]  lr: 5.6942e-06  eta: 0:13:24  time: 1.3892  data_time: 0.5012  memory: 10226  loss: 1.6928\n04/13 11:18:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1330/2022]  lr: 5.5502e-06  eta: 0:13:12  time: 0.8917  data_time: 0.0066  memory: 10226  loss: 1.6936\n04/13 11:18:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1340/2022]  lr: 5.4074e-06  eta: 0:12:59  time: 0.8892  data_time: 0.0066  memory: 10226  loss: 1.6664\n04/13 11:18:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1350/2022]  lr: 5.2657e-06  eta: 0:12:47  time: 1.1221  data_time: 0.2074  memory: 10226  loss: 1.8530\n04/13 11:19:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1360/2022]  lr: 5.1253e-06  eta: 0:12:35  time: 0.8815  data_time: 0.0067  memory: 10226  loss: 1.6598\n04/13 11:19:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1370/2022]  lr: 4.9861e-06  eta: 0:12:22  time: 0.8802  data_time: 0.0068  memory: 10226  loss: 1.5928\n04/13 11:19:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Iter(train) [1380/2022]  lr: 4.8482e-06  eta: 0:12:09  time: 0.8830  data_time: 0.0067  memory: 10226  loss: 1.7909\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Internlm2 1.8b Chat\n","metadata":{}},{"cell_type":"code","source":"!huggingface-cli download --resume-download internlm/internlm2-chat-1_8b --local-dir /kaggle/working/internlm2\nconfig_file = \"/kaggle/working/EmoLLM/xtuner_config/internlm2_7b_chat_qlora_e3.py\"\n\n\n# 打开并读取config.txt文件\nwith open(config_file, 'r') as file:\n    lines = file.readlines()\n\nlines[23] = 'pretrained_model_name_or_path = \"/kaggle/working/internlm2\"\\n'\nlines[27] = 'data_path = \"/kaggle/working/EmoLLM/datasets/data_pro.json\"\\n'  # 注意索引是从0开始\nlines[29] = 'max_length = 2048\\n'  # 注意索引是从0开始\nlines[33] = 'batch_size = 1  # per_device\\n'  # 注意索引是从0开始\nlines[34] = 'accumulative_counts = 16\\n'\n# lines[182] = 'load_from = None\\n'\n\n# 回写到config.txt文件\nwith open(config_file, 'w') as file:\n    file.writelines(lines)\n    \n# with open(config_file, 'r') as file:\n#      content = file.read()    \n# print(content)        ","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:56:43.913857Z","iopub.execute_input":"2024-04-13T11:56:43.914267Z","iopub.status.idle":"2024-04-13T11:56:52.548653Z","shell.execute_reply.started":"2024-04-13T11:56:43.914229Z","shell.execute_reply":"2024-04-13T11:56:52.547446Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\nFetching 14 files:   0%|                                 | 0/14 [00:00<?, ?it/s]downloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/generation_config.json to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/4b388c5f8fa584ebd865a2e9a6e5ced521a42646.incomplete\ndownloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/model.safetensors.index.json to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/e6c64e5f675cd66006d652b89a13230276991efa.incomplete\ndownloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/README.md to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/d5c1028274d46f2a54cdfbe1f5848090e9cc1b63.incomplete\ndownloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/config.json to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/4b646ab8d851ab57c50c58442e7a82ae37841729.incomplete\ndownloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/configuration_internlm2.py to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/b011dd3d125dea6e8d53dc494437dae688f3fc79.incomplete\ndownloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/.gitattributes to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/48462e4bc609e24ae114ee2ea77b9fe83ed7e2b3.incomplete\ndownloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/model-00001-of-00002.safetensors to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/c8617bf2394506ece1f8e02c2f054a503b67cf46d28d7e87dbdb437dc9bdc027.incomplete\n\nmodel.safetensors.index.json: 100%|████████| 13.7k/13.7k [00:00<00:00, 71.7MB/s]\u001b[A\n\nconfiguration_internlm2.py: 100%|██████████| 7.02k/7.02k [00:00<00:00, 50.5MB/s]\u001b[A\ndownloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/model-00002-of-00002.safetensors to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/4cb5fa37ac3f0c46748a9a8029c589e3c7a0b0ba9c1fa6bd225bcbbea80e9d24.incomplete\n\ngeneration_config.json: 100%|██████████████████| 132/132 [00:00<00:00, 1.26MB/s]\u001b[A\n\nREADME.md: 100%|███████████████████████████| 10.0k/10.0k [00:00<00:00, 39.1MB/s]\u001b[A\n\n.gitattributes: 100%|██████████████████████| 1.71k/1.71k [00:00<00:00, 13.2MB/s]\u001b[A\nFetching 14 files:   7%|█▊                       | 1/14 [00:00<00:03,  3.54it/s]downloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/tokenization_internlm2.py to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/ff53eba214b313c86ef489a823167a7ef2b52c09.incomplete\ndownloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/tokenizer.model to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/f868398fc4e05ee1e8aeba95ddf18ddcc45b8bce55d5093bead5bbf80429b48b.incomplete\ndownloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/special_tokens_map.json to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/1023d3510a8f6a72f071f934b9319d2b3dba616e.incomplete\n\nmodel-00001-of-00002.safetensors:   0%|             | 0.00/1.98G [00:00<?, ?B/s]\u001b[Adownloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/modeling_internlm2.py to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/e0fbc30623e0802e2a391da3a27c276ec113fda3.incomplete\ndownloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/tokenization_internlm2_fast.py to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/4d9d5f147c832320f1a96e4a7cb53b0c2c879d28.incomplete\n\n\nconfig.json: 100%|█████████████████████████████| 850/850 [00:00<00:00, 8.08MB/s]\u001b[A\u001b[A\nFetching 14 files:  21%|█████▎                   | 3/14 [00:00<00:01,  8.31it/s]\n\nmodel-00002-of-00002.safetensors:   0%|             | 0.00/1.80G [00:00<?, ?B/s]\u001b[A\u001b[A\n\n\ntokenization_internlm2.py: 100%|███████████| 8.81k/8.81k [00:00<00:00, 33.0MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nspecial_tokens_map.json: 100%|█████████████████| 713/713 [00:00<00:00, 6.43MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodeling_internlm2.py: 100%|███████████████| 60.0k/60.0k [00:00<00:00, 6.66MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:   1%|     | 21.0M/1.98G [00:00<00:11, 165MB/s]\u001b[A\n\n\ntokenization_internlm2_fast.py: 100%|██████| 7.80k/7.80k [00:00<00:00, 39.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\ntokenizer.model:   0%|                              | 0.00/1.48M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏    | 52.4M/1.98G [00:00<00:07, 243MB/s]\u001b[Adownloading https://huggingface.co/internlm/internlm2-chat-1_8b/resolve/ecccbb5c87079ad84e5788baa55dd6e21a9c614d/tokenizer_config.json to /root/.cache/huggingface/hub/models--internlm--internlm2-chat-1_8b/blobs/78f553cffe793f2867b6550c0db2c1a1bb8b9e52.incomplete\n\n\n\ntokenizer.model: 100%|█████████████████████| 1.48M/1.48M [00:00<00:00, 12.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00002.safetensors:   1%|    | 10.5M/1.80G [00:00<00:43, 41.3MB/s]\u001b[A\u001b[A\n\n\ntokenizer_config.json: 100%|███████████████| 2.51k/2.51k [00:00<00:00, 12.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:   5%|▏    | 94.4M/1.98G [00:00<00:06, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   6%|▍     | 126M/1.98G [00:00<00:06, 286MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:   2%|    | 31.5M/1.80G [00:00<00:19, 89.0MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▌     | 168M/1.98G [00:00<00:05, 304MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:   4%|▏    | 62.9M/1.80G [00:00<00:11, 150MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▋     | 210M/1.98G [00:00<00:05, 313MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:   6%|▎     | 105M/1.80G [00:00<00:08, 211MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▊     | 252M/1.98G [00:00<00:05, 319MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:   8%|▍     | 147M/1.80G [00:00<00:06, 252MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▉     | 294M/1.98G [00:00<00:05, 325MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  11%|▋     | 189M/1.80G [00:00<00:05, 278MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  17%|█     | 336M/1.98G [00:01<00:05, 328MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  13%|▊     | 231M/1.80G [00:01<00:05, 296MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  19%|█▏    | 377M/1.98G [00:01<00:04, 329MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  15%|▉     | 273M/1.80G [00:01<00:04, 307MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00002.safetensors:  18%|█     | 315M/1.80G [00:01<00:04, 314MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  21%|█▎    | 419M/1.98G [00:01<00:04, 319MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  20%|█▏    | 357M/1.80G [00:01<00:04, 317MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  23%|█▍    | 461M/1.98G [00:01<00:04, 321MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  22%|█▎    | 398M/1.80G [00:01<00:04, 322MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  25%|█▌    | 503M/1.98G [00:01<00:04, 324MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  25%|█▍    | 440M/1.80G [00:01<00:04, 324MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█▋    | 545M/1.98G [00:01<00:04, 325MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  27%|█▌    | 482M/1.80G [00:01<00:04, 326MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▊    | 587M/1.98G [00:01<00:04, 327MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  29%|█▊    | 524M/1.80G [00:01<00:03, 328MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  32%|█▉    | 629M/1.98G [00:02<00:04, 328MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  32%|█▉    | 566M/1.80G [00:02<00:03, 329MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  34%|██    | 671M/1.98G [00:02<00:03, 330MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  34%|██    | 608M/1.80G [00:02<00:03, 332MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  36%|██▏   | 713M/1.98G [00:02<00:03, 332MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  36%|██▏   | 650M/1.80G [00:02<00:03, 333MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  38%|██▎   | 755M/1.98G [00:02<00:03, 333MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  39%|██▎   | 692M/1.80G [00:02<00:03, 334MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  40%|██▍   | 797M/1.98G [00:02<00:03, 334MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|██▌   | 839M/1.98G [00:02<00:03, 315MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  41%|██▍   | 734M/1.80G [00:02<00:03, 306MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00002.safetensors:  43%|██▌   | 765M/1.80G [00:02<00:03, 306MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  44%|██▋   | 881M/1.98G [00:02<00:03, 314MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  45%|██▋   | 807M/1.80G [00:02<00:03, 314MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  47%|██▊   | 923M/1.98G [00:02<00:03, 308MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  47%|██▊   | 849M/1.80G [00:02<00:03, 299MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  48%|██▉   | 954M/1.98G [00:03<00:03, 295MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  50%|██▉   | 986M/1.98G [00:03<00:03, 288MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  50%|██▉   | 891M/1.80G [00:03<00:02, 306MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  51%|██▌  | 1.02G/1.98G [00:03<00:03, 289MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  51%|███   | 923M/1.80G [00:03<00:02, 302MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  53%|██▋  | 1.05G/1.98G [00:03<00:03, 286MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  54%|███▏  | 965M/1.80G [00:03<00:02, 303MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  55%|██▋  | 1.08G/1.98G [00:03<00:03, 286MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  55%|███▎  | 996M/1.80G [00:03<00:02, 304MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  56%|██▊  | 1.11G/1.98G [00:03<00:03, 284MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  58%|██▉  | 1.04G/1.80G [00:03<00:02, 306MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  58%|██▉  | 1.14G/1.98G [00:03<00:03, 271MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  60%|██▉  | 1.07G/1.80G [00:03<00:02, 305MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  59%|██▉  | 1.17G/1.98G [00:03<00:02, 270MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  62%|███  | 1.11G/1.80G [00:03<00:02, 315MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  61%|███  | 1.21G/1.98G [00:03<00:02, 275MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  64%|███▏ | 1.15G/1.80G [00:03<00:02, 311MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  62%|███  | 1.24G/1.98G [00:04<00:02, 281MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  66%|███▎ | 1.18G/1.80G [00:04<00:01, 309MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  65%|███▏ | 1.28G/1.98G [00:04<00:02, 298MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  68%|███▍ | 1.23G/1.80G [00:04<00:01, 315MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  67%|███▎ | 1.32G/1.98G [00:04<00:02, 308MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  71%|███▌ | 1.27G/1.80G [00:04<00:01, 317MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  69%|███▍ | 1.36G/1.98G [00:04<00:01, 311MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  73%|███▋ | 1.31G/1.80G [00:04<00:01, 317MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  71%|███▌ | 1.41G/1.98G [00:04<00:01, 315MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  75%|███▊ | 1.35G/1.80G [00:04<00:01, 319MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  73%|███▋ | 1.44G/1.98G [00:04<00:01, 305MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  78%|███▉ | 1.39G/1.80G [00:04<00:01, 317MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  75%|███▋ | 1.48G/1.98G [00:04<00:01, 318MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  80%|███▉ | 1.44G/1.80G [00:04<00:01, 321MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  77%|███▊ | 1.52G/1.98G [00:04<00:01, 316MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  82%|████ | 1.48G/1.80G [00:04<00:01, 303MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  79%|███▉ | 1.56G/1.98G [00:05<00:01, 293MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  84%|████▏| 1.51G/1.80G [00:05<00:00, 297MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  80%|████ | 1.59G/1.98G [00:05<00:01, 274MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  86%|████▎| 1.54G/1.80G [00:05<00:00, 294MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  82%|████ | 1.63G/1.98G [00:05<00:01, 269MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  88%|████▍| 1.57G/1.80G [00:05<00:00, 279MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  84%|████▏| 1.66G/1.98G [00:05<00:01, 268MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  89%|████▍| 1.60G/1.80G [00:05<00:00, 267MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  85%|████▎| 1.69G/1.98G [00:05<00:01, 270MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  91%|████▌| 1.64G/1.80G [00:05<00:00, 263MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  87%|████▎| 1.72G/1.98G [00:05<00:00, 272MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  93%|████▋| 1.67G/1.80G [00:05<00:00, 262MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  88%|████▍| 1.75G/1.98G [00:05<00:00, 275MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  95%|████▋| 1.70G/1.80G [00:05<00:00, 267MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  90%|████▍| 1.78G/1.98G [00:05<00:00, 278MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  97%|████▊| 1.74G/1.80G [00:05<00:00, 279MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  92%|████▌| 1.81G/1.98G [00:06<00:00, 283MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  93%|████▋| 1.85G/1.98G [00:06<00:00, 285MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  99%|████▉| 1.77G/1.80G [00:06<00:00, 272MB/s]\u001b[A\u001b[A\nmodel-00002-of-00002.safetensors: 100%|█████| 1.80G/1.80G [00:06<00:00, 291MB/s]\u001b[A\n\nmodel-00001-of-00002.safetensors:  97%|████▊| 1.92G/1.98G [00:06<00:00, 300MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  98%|████▉| 1.95G/1.98G [00:06<00:00, 303MB/s]\u001b[A\nmodel-00001-of-00002.safetensors: 100%|█████| 1.98G/1.98G [00:06<00:00, 300MB/s]\u001b[A\nFetching 14 files: 100%|████████████████████████| 14/14 [00:06<00:00,  2.00it/s]\n/kaggle/working/internlm2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install lmdeploy==0.3.0","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:47:49.754924Z","iopub.execute_input":"2024-04-13T11:47:49.755824Z","iopub.status.idle":"2024-04-13T11:48:43.426835Z","shell.execute_reply.started":"2024-04-13T11:47:49.755790Z","shell.execute_reply":"2024-04-13T11:48:43.425949Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting lmdeploy==0.3.0\n  Downloading lmdeploy-0.3.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from lmdeploy==0.3.0) (0.108.0)\nCollecting fire (from lmdeploy==0.3.0)\n  Downloading fire-0.6.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting mmengine-lite (from lmdeploy==0.3.0)\n  Downloading mmengine_lite-0.10.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lmdeploy==0.3.0) (1.26.4)\nCollecting peft<=0.9.0 (from lmdeploy==0.3.0)\n  Downloading peft-0.9.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from lmdeploy==0.3.0) (9.5.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from lmdeploy==0.3.0) (3.20.3)\nRequirement already satisfied: pydantic>2.0.0 in /opt/conda/lib/python3.10/site-packages (from lmdeploy==0.3.0) (2.5.3)\nRequirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from lmdeploy==0.3.0) (11.4.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from lmdeploy==0.3.0) (0.4.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from lmdeploy==0.3.0) (0.2.0)\nCollecting shortuuid (from lmdeploy==0.3.0)\n  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\nCollecting tiktoken (from lmdeploy==0.3.0)\n  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: torch<=2.1.2,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from lmdeploy==0.3.0) (2.1.2)\nCollecting transformers<=4.38.2,>=4.33.0 (from lmdeploy==0.3.0)\n  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton<=2.2.0,>=2.1.0 (from lmdeploy==0.3.0)\n  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from lmdeploy==0.3.0) (0.25.0)\nCollecting nvidia-nccl-cu12 (from lmdeploy==0.3.0)\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cuda-runtime-cu12 (from lmdeploy==0.3.0)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cublas-cu12 (from lmdeploy==0.3.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12 (from lmdeploy==0.3.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft<=0.9.0->lmdeploy==0.3.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft<=0.9.0->lmdeploy==0.3.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft<=0.9.0->lmdeploy==0.3.0) (6.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft<=0.9.0->lmdeploy==0.3.0) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft<=0.9.0->lmdeploy==0.3.0) (0.28.0)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft<=0.9.0->lmdeploy==0.3.0) (0.22.2)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>2.0.0->lmdeploy==0.3.0) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>2.0.0->lmdeploy==0.3.0) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>2.0.0->lmdeploy==0.3.0) (4.9.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<=2.1.2,>=2.0.0->lmdeploy==0.3.0) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<=2.1.2,>=2.0.0->lmdeploy==0.3.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<=2.1.2,>=2.0.0->lmdeploy==0.3.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<=2.1.2,>=2.0.0->lmdeploy==0.3.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch<=2.1.2,>=2.0.0->lmdeploy==0.3.0) (2024.2.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.38.2,>=4.33.0->lmdeploy==0.3.0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers<=4.38.2,>=4.33.0->lmdeploy==0.3.0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.38.2,>=4.33.0->lmdeploy==0.3.0) (0.15.2)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->lmdeploy==0.3.0) (0.32.0.post1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->lmdeploy==0.3.0) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->lmdeploy==0.3.0) (2.4.0)\nCollecting addict (from mmengine-lite->lmdeploy==0.3.0)\n  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from mmengine-lite->lmdeploy==0.3.0) (13.7.0)\nRequirement already satisfied: yapf in /opt/conda/lib/python3.10/site-packages (from mmengine-lite->lmdeploy==0.3.0) (0.40.2)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->lmdeploy==0.3.0) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->lmdeploy==0.3.0) (0.14.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft<=0.9.0->lmdeploy==0.3.0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers<=4.38.2,>=4.33.0->lmdeploy==0.3.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers<=4.38.2,>=4.33.0->lmdeploy==0.3.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers<=4.38.2,>=4.33.0->lmdeploy==0.3.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers<=4.38.2,>=4.33.0->lmdeploy==0.3.0) (2024.2.2)\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.33.0,>=0.29.0->fastapi->lmdeploy==0.3.0) (4.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<=2.1.2,>=2.0.0->lmdeploy==0.3.0) (2.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->mmengine-lite->lmdeploy==0.3.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->mmengine-lite->lmdeploy==0.3.0) (2.17.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<=2.1.2,>=2.0.0->lmdeploy==0.3.0) (1.3.0)\nRequirement already satisfied: importlib-metadata>=6.6.0 in /opt/conda/lib/python3.10/site-packages (from yapf->mmengine-lite->lmdeploy==0.3.0) (6.11.0)\nRequirement already satisfied: platformdirs>=3.5.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmengine-lite->lmdeploy==0.3.0) (4.2.0)\nRequirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from yapf->mmengine-lite->lmdeploy==0.3.0) (2.0.1)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi->lmdeploy==0.3.0) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi->lmdeploy==0.3.0) (1.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=6.6.0->yapf->mmengine-lite->lmdeploy==0.3.0) (3.17.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->mmengine-lite->lmdeploy==0.3.0) (0.1.2)\nDownloading lmdeploy-0.3.0-cp310-cp310-manylinux2014_x86_64.whl (78.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.9.0-py3-none-any.whl (190 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading mmengine_lite-0.10.3-py3-none-any.whl (451 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m451.8/451.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\nDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=66412f2d25d7a74bf6adb4f819264c6f462e6d0aeec6e84e44a1c4c6e8d9f1f2\n  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\nSuccessfully built fire\nInstalling collected packages: addict, triton, shortuuid, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cuda-runtime-cu12, nvidia-cublas-cu12, fire, tiktoken, mmengine-lite, transformers, peft, lmdeploy\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\nSuccessfully installed addict-2.4.0 fire-0.6.0 lmdeploy-0.3.0 mmengine-lite-0.10.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-runtime-cu12-12.4.127 nvidia-curand-cu12-10.3.5.147 nvidia-nccl-cu12-2.21.5 peft-0.9.0 shortuuid-1.0.13 tiktoken-0.6.0 transformers-4.38.2 triton-2.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!NPROC_PER_NODE=2 xtuner train /kaggle/working/EmoLLM/xtuner_config/internlm2_7b_chat_qlora_e3.py --deepspeed deepspeed_zero2","metadata":{"execution":{"iopub.status.busy":"2024-04-13T12:00:44.855349Z","iopub.execute_input":"2024-04-13T12:00:44.855733Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"04/13 12:00:47 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Use random port: 22119\n[2024-04-13 12:00:50,090] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2024-04-13 12:00:54,413] torch.distributed.run: [WARNING] \n[2024-04-13 12:00:54,413] torch.distributed.run: [WARNING] *****************************************\n[2024-04-13 12:00:54,413] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n[2024-04-13 12:00:54,413] torch.distributed.run: [WARNING] *****************************************\n[2024-04-13 12:00:58,897] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2024-04-13 12:00:59,235] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n  warnings.warn(\n[2024-04-13 12:01:01,318] [INFO] [comm.py:637:init_distributed] cdb=None\n[2024-04-13 12:01:01,319] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n/opt/conda/lib/python3.10/site-packages/mmengine/utils/dl_utils/setup_env.py:56: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n  warnings.warn(\n[2024-04-13 12:01:01,746] [INFO] [comm.py:637:init_distributed] cdb=None\n04/13 12:01:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n------------------------------------------------------------\nSystem environment:\n    sys.platform: linux\n    Python: 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\n    CUDA available: True\n    MUSA available: False\n    numpy_random_seed: 592459183\n    GPU 0,1: Tesla T4\n    CUDA_HOME: /usr/local/cuda\n    NVCC: Cuda compilation tools, release 12.1, V12.1.105\n    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n    PyTorch: 2.1.2\n    PyTorch compiling details: PyTorch built with:\n  - GCC 9.4\n  - C++ Version: 201703\n  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications\n  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)\n  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n  - LAPACK is enabled (usually provided by MKL)\n  - NNPACK is enabled\n  - CPU capability usage: AVX2\n  - CUDA Runtime 12.1\n  - NVCC architecture flags: -gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_70,code=compute_70;-gencode;arch=compute_75,code=compute_75\n  - CuDNN 8.9\n  - Magma 2.6.1\n  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.0, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.2, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n\n    TorchVision: 0.16.2\n    OpenCV: 4.9.0\n    MMEngine: 0.10.3\n\nRuntime environment:\n    launcher: pytorch\n    randomness: {'seed': None, 'deterministic': False}\n    cudnn_benchmark: False\n    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n    dist_cfg: {'backend': 'nccl'}\n    seed: None\n    deterministic: False\n    Distributed launcher: pytorch\n    Distributed training: True\n    GPU number: 2\n------------------------------------------------------------\n\n04/13 12:01:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\nSYSTEM = '现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。'\naccumulative_counts = 16\nalpaca_en = dict(\n    dataset=dict(\n        data_files=dict(train='/kaggle/working/EmoLLM/datasets/data_pro.json'),\n        path='json',\n        type='datasets.load_dataset'),\n    dataset_map_fn=None,\n    max_length=2048,\n    pack_to_max_length=True,\n    remove_unused_columns=True,\n    shuffle_before_pack=True,\n    template_map_fn=dict(\n        template='xtuner.utils.PROMPT_TEMPLATE.internlm2_chat',\n        type='xtuner.dataset.map_fns.template_map_fn_factory'),\n    tokenizer=dict(\n        padding_side='right',\n        pretrained_model_name_or_path='/kaggle/working/internlm2',\n        trust_remote_code=True,\n        type='transformers.AutoTokenizer.from_pretrained'),\n    type='xtuner.dataset.process_hf_dataset')\nbatch_size = 1\nbetas = (\n    0.9,\n    0.999,\n)\ncustom_hooks = [\n    dict(\n        tokenizer=dict(\n            padding_side='right',\n            pretrained_model_name_or_path='/kaggle/working/internlm2',\n            trust_remote_code=True,\n            type='transformers.AutoTokenizer.from_pretrained'),\n        type='xtuner.engine.DatasetInfoHook'),\n    dict(\n        evaluation_inputs=[\n            '我最近总是感到很焦虑，尤其是在学业上。我有个特别崇拜的同学，他好像在各方面都比我优秀，我总觉得自己怎么努力也追不上他，这让我压力特别大。',\n            '我知道应该理性看待，但就是忍不住会去比较。我甚至晚上会因为这个睡不着觉，总想着怎样才能像他那样出色。',\n        ],\n        every_n_iters=500,\n        prompt_template='xtuner.utils.PROMPT_TEMPLATE.internlm2_chat',\n        system='现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。',\n        tokenizer=dict(\n            padding_side='right',\n            pretrained_model_name_or_path='/kaggle/working/internlm2',\n            trust_remote_code=True,\n            type='transformers.AutoTokenizer.from_pretrained'),\n        type='xtuner.engine.EvaluateChatHook'),\n]\ndata_path = '/kaggle/working/EmoLLM/datasets/data_pro.json'\ndataloader_num_workers = 0\ndefault_hooks = dict(\n    checkpoint=dict(interval=1, type='mmengine.hooks.CheckpointHook'),\n    logger=dict(interval=10, type='mmengine.hooks.LoggerHook'),\n    param_scheduler=dict(type='mmengine.hooks.ParamSchedulerHook'),\n    sampler_seed=dict(type='mmengine.hooks.DistSamplerSeedHook'),\n    timer=dict(type='mmengine.hooks.IterTimerHook'))\nenv_cfg = dict(\n    cudnn_benchmark=False,\n    dist_cfg=dict(backend='nccl'),\n    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\nevaluation_freq = 500\nevaluation_inputs = [\n    '我最近总是感到很焦虑，尤其是在学业上。我有个特别崇拜的同学，他好像在各方面都比我优秀，我总觉得自己怎么努力也追不上他，这让我压力特别大。',\n    '我知道应该理性看待，但就是忍不住会去比较。我甚至晚上会因为这个睡不着觉，总想着怎样才能像他那样出色。',\n]\nlauncher = 'pytorch'\nload_from = None\nlog_level = 'INFO'\nlr = 0.0002\nmax_epochs = 3\nmax_length = 2048\nmax_norm = 1\nmodel = dict(\n    llm=dict(\n        pretrained_model_name_or_path='/kaggle/working/internlm2',\n        quantization_config=dict(\n            bnb_4bit_compute_dtype='torch.float16',\n            bnb_4bit_quant_type='nf4',\n            bnb_4bit_use_double_quant=True,\n            llm_int8_has_fp16_weight=False,\n            llm_int8_threshold=6.0,\n            load_in_4bit=True,\n            load_in_8bit=False,\n            type='transformers.BitsAndBytesConfig'),\n        torch_dtype='torch.float16',\n        trust_remote_code=True,\n        type='transformers.AutoModelForCausalLM.from_pretrained'),\n    lora=dict(\n        bias='none',\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,\n        task_type='CAUSAL_LM',\n        type='peft.LoraConfig'),\n    type='xtuner.model.SupervisedFinetune')\noptim_type = 'torch.optim.AdamW'\noptim_wrapper = dict(\n    optimizer=dict(\n        betas=(\n            0.9,\n            0.999,\n        ),\n        lr=0.0002,\n        type='torch.optim.AdamW',\n        weight_decay=0),\n    type='DeepSpeedOptimWrapper')\npack_to_max_length = True\nparam_scheduler = [\n    dict(\n        begin=0,\n        by_epoch=True,\n        convert_to_iter_based=True,\n        end=0.09,\n        start_factor=1e-05,\n        type='mmengine.optim.LinearLR'),\n    dict(\n        T_max=3,\n        begin=0.09,\n        by_epoch=True,\n        convert_to_iter_based=True,\n        eta_min=0.0,\n        type='mmengine.optim.CosineAnnealingLR'),\n]\npretrained_model_name_or_path = '/kaggle/working/internlm2'\nprompt_template = 'xtuner.utils.PROMPT_TEMPLATE.internlm2_chat'\nrandomness = dict(deterministic=False, seed=None)\nresume = False\nrunner_type = 'FlexibleRunner'\nstrategy = dict(\n    config=dict(\n        bf16=dict(enabled=False),\n        fp16=dict(enabled=True, initial_scale_power=16),\n        gradient_accumulation_steps='auto',\n        gradient_clipping='auto',\n        train_micro_batch_size_per_gpu='auto',\n        zero_allow_untested_optimizer=True,\n        zero_force_ds_cpu_optimizer=False,\n        zero_optimization=dict(overlap_comm=True, stage=2)),\n    exclude_frozen_parameters=True,\n    gradient_accumulation_steps=16,\n    gradient_clipping=1,\n    sequence_parallel_size=1,\n    train_micro_batch_size_per_gpu=1,\n    type='xtuner.engine.DeepSpeedStrategy')\ntokenizer = dict(\n    padding_side='right',\n    pretrained_model_name_or_path='/kaggle/working/internlm2',\n    trust_remote_code=True,\n    type='transformers.AutoTokenizer.from_pretrained')\ntrain_cfg = dict(by_epoch=True, max_epochs=3, val_interval=1)\ntrain_dataloader = dict(\n    batch_size=1,\n    collate_fn=dict(type='xtuner.dataset.collate_fns.default_collate_fn'),\n    dataset=dict(\n        dataset=dict(\n            data_files=dict(\n                train='/kaggle/working/EmoLLM/datasets/data_pro.json'),\n            path='json',\n            type='datasets.load_dataset'),\n        dataset_map_fn=None,\n        max_length=2048,\n        pack_to_max_length=True,\n        remove_unused_columns=True,\n        shuffle_before_pack=True,\n        template_map_fn=dict(\n            template='xtuner.utils.PROMPT_TEMPLATE.internlm2_chat',\n            type='xtuner.dataset.map_fns.template_map_fn_factory'),\n        tokenizer=dict(\n            padding_side='right',\n            pretrained_model_name_or_path='/kaggle/working/internlm2',\n            trust_remote_code=True,\n            type='transformers.AutoTokenizer.from_pretrained'),\n        type='xtuner.dataset.process_hf_dataset'),\n    num_workers=0,\n    sampler=dict(shuffle=True, type='mmengine.dataset.DefaultSampler'))\nvisualizer = None\nwarmup_ratio = 0.03\nweight_decay = 0\nwork_dir = './work_dirs/internlm2_7b_chat_qlora_e3'\n\n04/13 12:01:05 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmengine\" in the \"builder\" registry tree. As a workaround, the current \"builder\" registry in \"xtuner\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmengine\" is a correct scope, or whether the registry is initialized.\n04/13 12:01:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\nbefore_run:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(BELOW_NORMAL) LoggerHook                         \n -------------------- \nbefore_train:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) IterTimerHook                      \n(NORMAL      ) DatasetInfoHook                    \n(LOW         ) EvaluateChatHook                   \n(VERY_LOW    ) CheckpointHook                     \n -------------------- \nbefore_train_epoch:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) IterTimerHook                      \n(NORMAL      ) DistSamplerSeedHook                \n -------------------- \nbefore_train_iter:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) IterTimerHook                      \n -------------------- \nafter_train_iter:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) IterTimerHook                      \n(BELOW_NORMAL) LoggerHook                         \n(LOW         ) ParamSchedulerHook                 \n(LOW         ) EvaluateChatHook                   \n(VERY_LOW    ) CheckpointHook                     \n -------------------- \nafter_train_epoch:\n(NORMAL      ) IterTimerHook                      \n(LOW         ) ParamSchedulerHook                 \n(VERY_LOW    ) CheckpointHook                     \n -------------------- \nbefore_val:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) DatasetInfoHook                    \n -------------------- \nbefore_val_epoch:\n(NORMAL      ) IterTimerHook                      \n -------------------- \nbefore_val_iter:\n(NORMAL      ) IterTimerHook                      \n -------------------- \nafter_val_iter:\n(NORMAL      ) IterTimerHook                      \n(BELOW_NORMAL) LoggerHook                         \n -------------------- \nafter_val_epoch:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) IterTimerHook                      \n(BELOW_NORMAL) LoggerHook                         \n(LOW         ) ParamSchedulerHook                 \n(VERY_LOW    ) CheckpointHook                     \n -------------------- \nafter_val:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(LOW         ) EvaluateChatHook                   \n -------------------- \nafter_train:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(LOW         ) EvaluateChatHook                   \n(VERY_LOW    ) CheckpointHook                     \n -------------------- \nbefore_test:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) DatasetInfoHook                    \n -------------------- \nbefore_test_epoch:\n(NORMAL      ) IterTimerHook                      \n -------------------- \nbefore_test_iter:\n(NORMAL      ) IterTimerHook                      \n -------------------- \nafter_test_iter:\n(NORMAL      ) IterTimerHook                      \n(BELOW_NORMAL) LoggerHook                         \n -------------------- \nafter_test_epoch:\n(VERY_HIGH   ) RuntimeInfoHook                    \n(NORMAL      ) IterTimerHook                      \n(BELOW_NORMAL) LoggerHook                         \n -------------------- \nafter_test:\n(VERY_HIGH   ) RuntimeInfoHook                    \n -------------------- \nafter_run:\n(BELOW_NORMAL) LoggerHook                         \n -------------------- \n04/13 12:01:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - xtuner_dataset_timeout = 0:30:00\nGenerating train split: 6281 examples [00:00, 20205.82 examples/s]\nMap (num_proc=32): 100%|███████████| 6281/6281 [00:01<00:00, 5848.74 examples/s]\nFilter (num_proc=32): 100%|████████| 6281/6281 [00:00<00:00, 9518.18 examples/s]\nMap (num_proc=32): 100%|████████████| 6281/6281 [00:10<00:00, 591.20 examples/s]\nFilter (num_proc=32): 100%|████████| 6281/6281 [00:01<00:00, 3227.29 examples/s]\nFlattening the indices (num_proc=32): 100%|█| 6281/6281 [00:00<00:00, 10104.11 e\nMap (num_proc=32): 100%|███████████| 6281/6281 [00:02<00:00, 2391.66 examples/s]\nMap (num_proc=32): 100%|████████████| 1329/1329 [00:01<00:00, 858.32 examples/s]\n04/13 12:01:34 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Dataset Dataset has no metainfo. ``dataset_meta`` in visualizer will be None.\nquantization_config convert to <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>\nquantization_config convert to <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.45s/it]\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:05<00:00,  2.52s/it]\n04/13 12:01:42 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Due to the implementation of the PyTorch version of flash attention, even when the `output_attentions` flag is set to True, it is not possible to return the `attn_weights`.\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - dispatch internlm2 attn forward\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n04/13 12:01:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - replace internlm2 rope\n[2024-04-13 12:01:44,077] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n[2024-04-13 12:01:44,275] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n[2024-04-13 12:01:44,280] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n[2024-04-13 12:01:44,280] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n[2024-04-13 12:01:44,320] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n[2024-04-13 12:01:44,320] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\n[2024-04-13 12:01:44,320] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n[2024-04-13 12:01:44,320] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500,000,000\n[2024-04-13 12:01:44,320] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500,000,000\n[2024-04-13 12:01:44,320] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n[2024-04-13 12:01:44,320] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n[2024-04-13 12:01:44,776] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n[2024-04-13 12:01:44,777] [INFO] [utils.py:801:see_memory_usage] MA 1.69 GB         Max_MA 1.75 GB         CA 1.8 GB         Max_CA 2 GB \n[2024-04-13 12:01:44,777] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 7.39 GB, percent = 23.6%\n[2024-04-13 12:01:44,968] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n[2024-04-13 12:01:44,969] [INFO] [utils.py:801:see_memory_usage] MA 1.69 GB         Max_MA 1.82 GB         CA 1.93 GB         Max_CA 2 GB \n[2024-04-13 12:01:44,969] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 7.39 GB, percent = 23.6%\n[2024-04-13 12:01:44,969] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n[2024-04-13 12:01:45,162] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n[2024-04-13 12:01:45,164] [INFO] [utils.py:801:see_memory_usage] MA 1.69 GB         Max_MA 1.69 GB         CA 1.93 GB         Max_CA 2 GB \n[2024-04-13 12:01:45,164] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 7.4 GB, percent = 23.6%\n[2024-04-13 12:01:45,168] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW\n[2024-04-13 12:01:45,168] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n[2024-04-13 12:01:45,168] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n[2024-04-13 12:01:45,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]\n[2024-04-13 12:01:45,172] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n[2024-04-13 12:01:45,173] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n    \"partition_activations\": false, \n    \"contiguous_memory_optimization\": false, \n    \"cpu_checkpointing\": false, \n    \"number_checkpoints\": null, \n    \"synchronize_checkpoint_boundary\": false, \n    \"profile\": false\n}\n[2024-04-13 12:01:45,173] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n[2024-04-13 12:01:45,173] [INFO] [config.py:1000:print]   amp_enabled .................. False\n[2024-04-13 12:01:45,173] [INFO] [config.py:1000:print]   amp_params ................... False\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   autotuning_config ............ {\n    \"enabled\": false, \n    \"start_step\": null, \n    \"end_step\": null, \n    \"metric_path\": null, \n    \"arg_mappings\": null, \n    \"metric\": \"throughput\", \n    \"model_info\": null, \n    \"results_dir\": \"autotuning_results\", \n    \"exps_dir\": \"autotuning_exps\", \n    \"overwrite\": true, \n    \"fast\": true, \n    \"start_profile_step\": 3, \n    \"end_profile_step\": 5, \n    \"tuner_type\": \"gridsearch\", \n    \"tuner_early_stopping\": 5, \n    \"tuner_num_trials\": 50, \n    \"model_info_path\": null, \n    \"mp_size\": 1, \n    \"max_train_batch_size\": null, \n    \"min_train_batch_size\": 1, \n    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n    \"min_train_micro_batch_size_per_gpu\": 1, \n    \"num_tuning_micro_batch_sizes\": 3\n}\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7be55907d0f0>\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   communication_data_type ...... None\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n[2024-04-13 12:01:45,174] [INFO] [config.py:1000:print]   disable_allgather ............ False\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   dump_state ................... False\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n    \"enabled\": false, \n    \"recompute_fwd_factor\": 0.0, \n    \"profile_step\": 1, \n    \"module_depth\": -1, \n    \"top_modules\": 1, \n    \"detailed\": true, \n    \"output_file\": null\n}\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   fp16_auto_cast ............... False\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   fp16_enabled ................. True\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   global_rank .................. 0\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 16\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   gradient_clipping ............ 1\n[2024-04-13 12:01:45,175] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   loss_scale ................... 0\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   nebula_config ................ {\n    \"enabled\": false, \n    \"persistent_storage_path\": null, \n    \"persistent_time_interval\": 100, \n    \"num_of_version_in_retention\": 2, \n    \"enable_nebula_load\": true, \n    \"load_path\": null\n}\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   optimizer_name ............... None\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   optimizer_params ............. None\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   pld_enabled .................. False\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   pld_params ................... False\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   scheduler_name ............... None\n[2024-04-13 12:01:45,176] [INFO] [config.py:1000:print]   scheduler_params ............. None\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   sparse_attention ............. None\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   steps_per_print .............. 10000000000000\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   world_size ................... 2\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   zero_enabled ................. True\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. False\n[2024-04-13 12:01:45,177] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n[2024-04-13 12:01:45,177] [INFO] [config.py:986:print_user_config]   json = {\n    \"gradient_accumulation_steps\": 16, \n    \"train_micro_batch_size_per_gpu\": 1, \n    \"gradient_clipping\": 1, \n    \"zero_allow_untested_optimizer\": true, \n    \"zero_force_ds_cpu_optimizer\": false, \n    \"zero_optimization\": {\n        \"stage\": 2, \n        \"overlap_comm\": true\n    }, \n    \"fp16\": {\n        \"enabled\": true, \n        \"initial_scale_power\": 16\n    }, \n    \"bf16\": {\n        \"enabled\": false\n    }, \n    \"steps_per_print\": 1.000000e+13\n}\n04/13 12:01:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Num train samples 1329\n04/13 12:01:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - train example:\n2024-04-13 12:01:46.712325: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-13 12:01:46.712345: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-13 12:01:46.712399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-13 12:01:46.712469: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-13 12:01:46.872042: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-04-13 12:01:46.872050: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n04/13 12:01:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - <s><|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n医生，我最近总是感到很焦虑，尤其是当我想到我即将参加的一个重要会议。我担心我会表现得很糟糕，我甚至开始失眠了。\n\n<|im_end|>\n<|im_start|>assistant\n我理解你的感受，面对重要的场合感到焦虑是很常见的。首先，我想告诉你，你的感受是完全可以理解的。你能告诉我更多关于这个会议的情况吗？这样我可以更好地帮助你。\n\n<|im_end|>\n<|im_start|>user\n这个会议对我来说非常重要，因为我需要向一群高层管理人员展示我的项目。我总是担心我会忘词，或者我的报告不够好。\n\n<|im_end|>\n<|im_start|>assistant\n听起来你对自己的要求很高，这是成功的一个关键因素，但同时也可能是导致你焦虑的原因。让我们一步步来。首先，能否告诉我你觉得最令你担心的是什么？是忘词，还是认为报告不够完美？\n\n<|im_end|>\n<|im_start|>user\n我觉得我最大的恐惧是忘词。一想到在那么多人面前突然大脑一片空白，我就感到紧张。\n\n<|im_end|>\n<|im_start|>assistant\n我明白。忘词是公共演讲时常见的担忧。我们可以通过一些技巧来减轻这种恐惧。例如，你可以准备一些关键点的提示卡，或者提前多次练习，这样你就能更加熟悉你的材料。你觉得这样的方法对你有帮助吗？\n\n<|im_end|>\n<|im_start|>user\n嗯，我想练习确实会有所帮助。但我还是担心，如果我在演讲中出现问题，我会感到非常尴尬。\n\n<|im_end|>\n<|im_start|>assistant\n这种担忧是很正常的。记住，每个人都会犯错，包括那些在公共场合演讲的高手。关键是学会接受不完美，并从中学习。你可以尝试一些放松技巧，比如深呼吸或者正念冥想，来帮助你控制紧张情绪。\n\n<|im_end|>\n<|im_start|>user\n我尝试过深呼吸，但有时候感觉不太管用。\n\n<|im_end|>\n<|im_start|>assistant\n深呼吸是一个很好的开始，但确实需要一些练习才能在紧张时刻有效运用。此外，你可以尝试在演讲前进行渐进性肌肉放松练习，这样可以帮助你的身体放松下来。同时，我想让你知道，感到焦虑并不意味着你会表现不佳。事实上，适度的紧张可以提高你的表现。\n\n<|im_end|>\n<|im_start|>user\n听你这么说，我感觉好多了。我应该怎么开始练习这些技巧呢？\n\n<|im_end|>\n<|im_start|>assistant\n你可以从每天花几分钟练习深呼吸和肌肉放松开始。对于演讲，你可以先在镜子前练习，然后逐渐过渡到在朋友或家人面前练习。每次练习后，给自己一些积极的反馈。记住，这是一个逐步的过程，不要对自己太过苛刻。如果你发现自己在某些时刻感到特别紧张，可以尝试立即停下来，深呼吸，然后继续。\n\n<|im_end|>\n<s><|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n医生，我最近在社交场合总是感到很紧张，尤其是和新认识的人交流时，我会不自觉地出汗，说话也结巴。\n\n<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，社交焦虑是很常见的。首先，我想告诉你，这并不是你一个人的问题。你能告诉我，这种紧张感通常在什么情况下会出现吗？\n\n<|im_end|>\n<|im_start|>user\n嗯，比如在聚会或者工作场合需要和陌生人交流时，我就会感到紧张。我真的很渴望能够自如地和他们交谈，但就是做不到。\n\n<|im_end|>\n<|im_start|>assistant\n明白了。其实，这种紧张和渴望是可以通过一些方法来缓解的。我想先教你一个深呼吸的方法，当你感到紧张时，可以尝试做几次深呼吸，把注意力转移到呼吸上，这样有助于你放松下来。\n\n<|im_end|>\n<|im_start|>user\n深呼吸吗？我试试看。\n\n<|im_end|>\n<|im_start|>assistant\n对，试试看。另外，你可以在社交场合前，提前准备一些话题，这样在交流时就不会感到那么紧张了。你可以从一些简单的话题开始，比如天气、兴趣爱好等。\n\n<|im_end|>\n<|im_start|>user\n嗯，这个方法听起来不错。但是，如果我在交谈过程中还是出现结巴、出汗的情况怎么办？\n\n<|im_end|>\n<|im_start|>assistant\n这时，你可以尝试用一些积极的自我暗示来调整心态。比如，你可以告诉自己：“我可以做到的，我能够顺利进行这次交流。”另外，不要过分关注自己的紧张表现，试着把注意力放在对方身上，关注对方的表情和回应。\n\n<|im_end|>\n<|im_start|>user\n好的，我会试试这些方法。但是，如果我还是觉得很难克服这种社交焦虑，我该怎么办？\n\n<|im_end|>\n<|im_start|>assistant\n如果你尝试了这些方法，但仍然觉得很难克服社交焦虑，那么我建议你寻求专业的心理辅导。心理辅导师会根据你的具体情况，为你提供更针对性的建议和治疗方案。记住，勇敢面对问题，寻求帮助是解决问题的第一步。\n\n<|im_end|>\n<s><|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n医生，我是一名大学生，最近在准备期末考试，但我发现自己对同学的学业压力感到非常同情，这让我也很难受。\n\n<|im_end|>\n<|im_start|>assistant\n我理解你的感受，面对他人的压力同时也会影响到自己。首先，你能告诉我，这种同情情感具体给你带来了哪些困扰吗？\n\n<|im_end|>\n<|im_start|>user\n我会因为担心他们而无法集中精力学习，有时候甚至会感到胸闷和焦虑。我知道我应该先照顾好自己，但这种感觉总是困扰着我。\n\n<|im_end|>\n<|im_start|>assistant\n你能够意识到这一点已经很棒了。在面对这种情况时，我们可以尝试一些方法来调整你的情绪。首先，当你感到焦虑时，可以尝试深呼吸，慢慢吸气，再慢慢呼气，重复几次。此外，你认为是什么原因让你如此关心同学的学业压力呢？\n\n<|im_end|>\n<|im_start|>user\n我觉得可能是因为我害怕他们承受不住压力，而我却无能为力。这种无助感让我很难受。\n\n<|im_end|>\n<|im_start|>assistant\n理解你的感受。在这种情况下，我们可以尝试改变你的思维方式。首先，要认识到每个人都有自己的承受能力和解决方法。你可以尝试与他们分享一些有效的学习方法和缓解压力的技巧，但最终他们需要自己去面对和解决问题。\n\n<|im_end|>\n<|im_start|>user\n那我该如何调整自己的心态，以便在关心同学的同时，不影响自己的学业呢？\n\n<|im_end|>\n<|im_start|>assistant\n这是一个很好的问题。你可以尝试以下方法：\n\n1. 设定界限：明确自己的责任范围，知道何时该关心他人，何时专注于自己的学业。\n2. 增强自我意识：了解自己的情绪，当发现自己过度关心他人时，及时调整心态。\n3. 培养同理心：学会站在他人的角度思考问题，但不要让这种情感影响到自己的情绪。\n\n最后，如果情况仍然无法改善，你可以寻求专业心理辅导，以便更好地解决这个问题。\n\n<|im_end|>\n<|im_start|>user\n谢谢医生，我会按照你的建议去尝试调整自己的心态。\n\n<|im_end|>\n<|im_start|>assistant\n不用谢，很高兴能帮到你。请记住，关注自己的心理健康同样重要。如果你在尝试这些方法的过程中遇到困难，随时欢迎回来咨询。祝你学业顺利，心理健康！<|im_end|>\n<s><|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n医生，我最近总是忍不住回想以前的工作时光，那时候的同事关系很和谐，工作也没有现在这么压力大。我现在的工作环境让我感到很压抑，我该怎么办？\n\n<|im_end|>\n<|im_start|>assistant\n我能理解你的感受，怀旧是一种很正常的情感，尤其是在面对压力时。首先，你能告诉我，现在的工作环境具体给你带来了哪些压力吗？这样我才能更好地帮助你。\n\n<|im_end|>\n<|im_start|>user\n现在的工作节奏太快，竞争激烈，我觉得自己跟不上大家的步伐。而且，同事之间似乎没有以前那么亲密，总是保持着一定的距离。\n\n<|im_end|>\n<|im_start|>assistant\n听起来你面临的主要是工作节奏和人际关系的问题。在这种情况下，我们可以尝试以下几个方法来缓解你的压力。首先，关于工作节奏，你可以尝试制定合理的工作计划，分清主次，不要给自己太大的压力。其次，在人际关系方面，你可以主动与同事沟通，分享你的感受，也许他们也有类似的经历。\n\n<|im_end|>\n<|im_start|>user\n可是我觉得自己很难迈出这一步，我害怕被拒绝。\n\n<|im_end|>\n<|im_start|>assistant\n这是很正常的担忧。你可以从一些小事做起，比如请同事喝杯咖啡，或者一起吃午饭。这样逐渐建立起信任，你们之间的关系也会变得更加亲密。同时，你也可以尝试回忆一下以前的工作场景，找出那些让你感到舒适和愉快的元素，然后尝试将这些元素融入到现在的工作中。\n\n<|im_end|>\n<|im_start|>user\n嗯，我明白了。那我该如何调整自己的心态，不再总是怀旧呢？\n\n<|im_end|>\n<|im_start|>assistant\n调整心态需要时间和耐心。你可以尝试以下几个方法：一是认识到每个阶段都有其独特的价值和意义，现在的经历同样值得珍惜；二是学会感恩，感谢现在的工作给你带来的成长和收获；三是保持乐观，相信自己能够适应并克服当前的困难。\n\n<|im_end|>\n<|im_start|>user\n谢谢你的建议，我会努力去实践的。\n\n<|im_end|>\n<|im_start|>assistant\n不用谢，很高兴能帮到你。如果在实践过程中遇到任何问题，都可以随时来找我。记住，心理健康很重要，要时刻关注并照顾好自己的内心。祝你工作顺利，生活愉快！<|im_end|>\n<s><|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n（小学生）医生，我最近老是担心考试考不好，晚上会睡不着觉，白天上课又没精神听讲。我是不是有病啊？<|im_end|>\n<|im_start|>assistant\n亲爱的同学，你并没有“生病”，只是可能有些过度焦虑了。这是很常见的现象，许多小朋友在面临考试时都会感到紧张。首先，我们要明白每个人都会遇到困难和挑战，这并不是你个人独有的问题。你可以试着在睡前做一些放松的活动，比如听轻柔的故事或者进行深呼吸练习来帮助入睡。同时，我们可以制定一个合理的学习计划，每天按时完成，这样你就不会过于担忧考试了。你觉得这样的建议对你有帮助吗？<|im_end|>\n<|im_start|>user\n嗯，我会试试的。但我还是觉得自己比其他同学笨，学东西\n04/13 12:01:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - before_train in EvaluateChatHook.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2756: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /root/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)\n  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n04/13 12:02:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<s><|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我最近总是感到很焦虑，尤其是在学业上。我有个特别崇拜的同学，他好像在各方面都比我优秀，我总觉得自己怎么努力也追不上他，这让我压力特别大。<|im_end|>\n<|im_start|>assistant\n我理解你现在的感受。面对比自己优秀的同学，我们常常会感到压力和焦虑。这种情绪就像是一阵风暴，让我们感到不安和困惑。\n\n但是，我想告诉你，每个人都有自己的独特之处，就像花园里的各种花朵一样。\n\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n04/13 12:02:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Sample output:\n<s><|im_start|>system\n现在你是一个心理专家，我有一些心理问题，请你用专业的知识帮我解决。<|im_end|>\n<|im_start|>user\n我知道应该理性看待，但就是忍不住会去比较。我甚至晚上会因为这个睡不着觉，总想着怎样才能像他那样出色。<|im_end|>\n<|im_start|>assistant\n我明白你的感受。比较是人类的天性，我们常常会不自觉地与他人比较，以寻求自我价值的满足。然而，这种比较往往会导致焦虑和自卑感。\n\n想象一下，你是一只小鸟，你渴望飞翔，但你的\n\n04/13 12:02:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n04/13 12:02:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n04/13 12:02:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /kaggle/working/work_dirs/internlm2_7b_chat_qlora_e3.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n04/13 12:02:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 10/665]  lr: 3.1036e-05  eta: 1:06:13  time: 2.0017  data_time: 0.0050  memory: 6156  loss: 2.3808\n04/13 12:02:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 20/665]  lr: 6.5519e-05  eta: 1:07:01  time: 2.0711  data_time: 0.0045  memory: 6419  loss: 2.3565\n04/13 12:03:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 30/665]  lr: 1.0000e-04  eta: 1:07:24  time: 2.1016  data_time: 0.0045  memory: 6419  loss: 2.3502\n04/13 12:03:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 40/665]  lr: 1.3448e-04  eta: 1:07:51  time: 2.1564  data_time: 0.0044  memory: 6419  loss: 2.1724\n04/13 12:03:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 50/665]  lr: 1.6897e-04  eta: 1:08:23  time: 2.2189  data_time: 0.0043  memory: 6419  loss: 2.2696\n04/13 12:04:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 60/665]  lr: 2.0000e-04  eta: 1:08:28  time: 2.1904  data_time: 0.0044  memory: 6419  loss: 2.1549\n04/13 12:04:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 70/665]  lr: 1.9999e-04  eta: 1:08:20  time: 2.1713  data_time: 0.0047  memory: 6419  loss: 2.1531\n04/13 12:04:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 80/665]  lr: 1.9995e-04  eta: 1:08:10  time: 2.1784  data_time: 0.0044  memory: 6419  loss: 2.1208\n04/13 12:05:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 90/665]  lr: 1.9989e-04  eta: 1:07:59  time: 2.1811  data_time: 0.0046  memory: 6419  loss: 2.0488\n04/13 12:05:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][100/665]  lr: 1.9980e-04  eta: 1:07:45  time: 2.1822  data_time: 0.0044  memory: 6419  loss: 2.0850\n04/13 12:06:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][110/665]  lr: 1.9969e-04  eta: 1:07:28  time: 2.1721  data_time: 0.0043  memory: 6419  loss: 2.0480\n04/13 12:06:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][120/665]  lr: 1.9955e-04  eta: 1:07:11  time: 2.1795  data_time: 0.0044  memory: 6419  loss: 2.2209\n04/13 12:06:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][130/665]  lr: 1.9939e-04  eta: 1:06:54  time: 2.1777  data_time: 0.0045  memory: 6419  loss: 2.0793\n04/13 12:07:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][140/665]  lr: 1.9921e-04  eta: 1:06:35  time: 2.1731  data_time: 0.0044  memory: 6419  loss: 2.0025\n04/13 12:07:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][150/665]  lr: 1.9900e-04  eta: 1:06:17  time: 2.1802  data_time: 0.0046  memory: 6419  loss: 2.0788\n04/13 12:07:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][160/665]  lr: 1.9876e-04  eta: 1:05:58  time: 2.1812  data_time: 0.0045  memory: 6419  loss: 2.0280\n04/13 12:08:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][170/665]  lr: 1.9850e-04  eta: 1:05:39  time: 2.1793  data_time: 0.0044  memory: 6419  loss: 1.8932\n04/13 12:08:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][180/665]  lr: 1.9822e-04  eta: 1:05:20  time: 2.1834  data_time: 0.0045  memory: 6419  loss: 1.9210\n04/13 12:08:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][190/665]  lr: 1.9791e-04  eta: 1:04:59  time: 2.1728  data_time: 0.0044  memory: 6419  loss: 1.9833\n","output_type":"stream"}]}]}